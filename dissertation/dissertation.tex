\documentclass[ms,electronic,twosidetoc,letterpaper,chaptercenter,parttop]{byumsphd}
% Author: Chris Monson
%
% This document is in the public domain
%
% Options for this class include the following (* indicates default):
%
%   phd (*) -- produce a dissertation
%   ms -- produce a thesis
%
%   electronic -- default official university option, overrides the following:
%                 - equalmargins
%
%   hardcopy -- overrides the following:
%                 - no equalmargins
%                 - twoside
%
%   letterpaper -- ignored, but helpful for the Makefile that I use
%
%   10pt -- 10 point font size
%   11pt -- 11 point font size
%   12pt (*) -- 12 point font size
%
%   lof -- produce a list of figures in the preamble (off)
%   lot -- produce a list of tables in the preamble (off)
%   lol -- produce a list of listings in the preamble (off)
%
%   layout -- show layout lines on the pages, helps with overfull boxes (off)
%   grid -- show a half-inch grid on every page, helps with printing (off)
%   separator -- print an extra instruction page between preamble and body (off)
%
%   twoside (*) -- two-sided output (margins alternate for odd and even pages,
%     blank pages inserted to ensure that chapters begin on the right side of a
%     bound copy, etc.)
%   oneside -- one-sided output (margins are the same on all pages)
%   equalmargins -- make all margins equal - ugly for binding, but compliant
%
%   twosidetoc - start two-sided margins at the TOC instead of the body.  This
%     is sometimes (oddly) required, but be aware that it will make the page
%     numbering seem screwy, e.g., the first four full sheets of paper will
%     have number i-iv (not shown, though), and the next sheets will each have
%     two numbers, one for each side.  I suspect that most people don't look at
%     the roman numerals anyway, but it is a weird requirement.
%
%   openright (*) -- force new chapters to start on an odd page
%   openany -- don't use this, it's ugly
%
%   prettyheadings -- make the section/chapter headings look nice
%   compliantheadings (*) -- make them look ugly, but compliant with standards
%
%   chaptercenter -- center the chapter headings horizontally
%   chapterleft (*) -- place chapter headings on the left
%
%   partmiddle -- Part headers are centered vertically, no other text on page
%   parttop (*) -- Part headers at top of page, other text expected
%
%   duplexprinter -- Ensures that the two-sided portion starts on the right
%     side when printing.  This is not for use in submission, since the best
%     thing to do there is to print everything out one-sided, then take it down
%     to the copy store to have them do the rest.  It does help to save trees
%     when you are printing out copies just to look at them and fiddle with
%     things.
%
%
% EXAMPLES:
%
% The rest is up to you.  To fiddle with margins, use the \settextwidth and
% \setbindingoffset macros, described below.  I suggest that you
% \settextwidth{6.0in} for better-looking output (otherwise you'll get 3/4-inch
% margins after binding, which is sort of weird).  This will depend on the
% opinions of the various dean/coordinator folks, though, so be sure to ask
% them before embarking on a major formatting task.

% The following command fixes my particular printer, which starts 0.03 inches
% too low, shifting the whole page down by that amount.  This shifts the
% document content up so that it comes out right when printed.
%
% Discovering this sort of behavior is best done by specifying the ``grid''
% option in the class parameters above.  It prints a 1/2 inch grid on every
% page.  You can then use a ruler to determine exactly what the printer is
% doing.
%
% Uncomment to shift content up (accounting for printer problems)
%\setlength{\voffset}{-.03in}

% Here we set things up for invisible hyperlinks in the document.  This makes
% the electronic version clickable without changing the way that the document
% prints.  It's useful, but optional.
%
% NOTE: "driverfallback=ps2pdf" chooses ps2pdf in the case of LaTeX and pdftex
% in the case of pdflatex. If you use my LaTeX makefile (at
% http://latex-makefile.googlecode.com/) then pdftex is the default There are
% many other benefits to using the makefile, too.  This option is not always
% available, so use with care.
\usepackage[
    bookmarks=true,
    bookmarksnumbered=true,
    breaklinks=false,
    raiselinks=true,
    pdfborder={0 0 0},
    colorlinks=false,
    plainpages=false,
    ]{hyperref}

% To fiddle with the margin settings use the below.  DO NOT change stuff
% directly (like setting \textwidth) - it will break subtle things and you'll
% be tearing your hair out.
%
% For example, if you want 1.5in equal margins, or 2in and 1in margins when
% printing, add the following below:
%
%\setbindingoffset{1.0in}
%\settextwidth{5.5in}
%
% When equalmargins is specified in the class options, the margins will be
% equal at 1.5in each: (8.5 - 5.5) / 2.  When equalmargins is not specified,
% the inner margin will be 2.0 and the outer margin will be 1.0: inner = (8.5 -
% 5.5 - 1.0) / 2 + 1.0 (the 1.0 is the binding offset).
%
% The idea is this: you determine how much space the text is going to take up,
% whether for an electronic document (equalmargins) or not.  You don't want the
% layout shifting around between printed and electronic documents.
%
% So, you specify the text width.  Then, if there is a binding offset (when
% binding your thesis, the binding takes up space - usually 0.5 inches), that
% reduces the visual space on the final printed copy.  So, the *effective*
% margins are calculated by reducing the page size by the binding offset, then
% computing the remaining space and dividing by two.  Adding back in the
% binding offset gives the inner margin.  The outer margin is just what's left.
%
% All of this is done using the geometry package, which should be manipulated
% directly at your peril.  It's best just to use the above macros to manipulate
% your margins.
%
% That said, using the geometry macro to set top and bottom margins, or
% anything else vertical, is perfectly safe and encouraged, e.g.,
%
%\geometry{top=2.0in,bottom=2.0in}
%
% Just don't fiddle with horizontal margins this way.  You have been warned.

% This makes hyperlinks point to the tops of figures, not their captions
\usepackage[all]{hypcap}

% These packages allow the bibliography to be sorted alphabetically and allow references to more than one paper to be sorted and compressed (i.e. instead of [5,2,4,6] you get [2,4-6])
\usepackage[numbers,sort&compress]{natbib}
\usepackage{hypernat}

% Because I use these things in more than one place, I created new commands for
% them.  I did not use \providecommand because I absolutely want LaTeX to error
% out if these already exist.
\newcommand{\Title}{A CPS-like Transformation of Continuation Marks}
\newcommand{\Author}{Kimball R. Germane}
\newcommand{\GraduationMonth}{September}
\newcommand{\GraduationYear}{2012}

% Set up the internal PDF information so that it becomes part of the document
% metadata.  The pdfinfo command will display this.
\hypersetup{%
    pdftitle=\Title,%
    pdfauthor=\Author,%
    pdfsubject={MS Dissertation, BYU CS Department: %
                Degree Granted \GraduationMonth~\GraduationYear, Document Created \today},%
    pdfkeywords={BYU, thesis, dissertation, LaTeX},%
}

% Rewrite the itemize, description, and enumerate environments to have more
% reasonable spacing:
\newcommand{\ItemSep}{\itemsep 0pt}
\let\oldenum=\enumerate
\renewcommand{\enumerate}{\oldenum \ItemSep}
\let\olditem=\itemize
\renewcommand{\itemize}{\olditem \ItemSep}
\let\olddesc=\description
\renewcommand{\description}{\olddesc \ItemSep}

% Important settings for the byumsphd class.
\title{\Title}
\author{\Author}

\committeechair{Jay McCarthy}
\committeemembera{Sean Warnick}
\committeememberb{Dennis Ng}
%\committeememberc{b}
%\committeememberd{c}

\monthgraduated{\GraduationMonth}
\yeargraduated{\GraduationYear}
\yearcopyrighted{\GraduationYear}

\documentabstract{%
Continuation marks are a programming language feature which generalize stack inspection.
Continuation marks currently lack a meaning-preserving transformation to the
$\lambda$-calculus, a useful and widely-used computation model. Such a transformation
would simplify the construction of compilers which treat continuation marks correctly. We
present a CPS-like transformation from the call-by-value $\lambda$-calculus augmented with
continuation marks to the pure call-by-value $\lambda$-calculus and verify that the
transformation indeed preserves meaning.
}

\documentkeywords{%
    continuation marks, continuation-passing style, Redex
}

\acknowledgments{%
Thanks to R.L. Stine for showing me that a chapter doesn't have to be more than two pages.

My wife is the hottest thing ever.
}

\department{Computer~Science}
\graduatecoordinator{Dan~Ventura}
\collegedean{Thomas~W.~Sederberg}
\collegedeantitle{Associate~Dean}

% Customize the name of the Table of Contents section.
\renewcommand\contentsname{Table of Contents}

% Remove all widows an orphans.  This is not normally recommended, but in a
% paper dissertation there is no reasonable way around it; you can't exactly
% rewrite already-published content to fix the problem.
\clubpenalty 10000
\widowpenalty 10000

% Allow pages to have extra blank space at the bottom in order to accommodate
% removal of widows and orphans.
\raggedbottom

% Produce nicely formatted paragraphs. There is nothing additional to do.  In
% case you get some problems, surround your text with
% \begin{sloppy} ... \end{sloppy}. If that does not work, try
% \microtypesetup{protrusion=false} ... \microtypesetup{protrusion=true}
\usepackage{microtype}

\usepackage{amsmath}
\usepackage{amsthm}
%\usepackage{slatex}

\begin{document}

% Produce the preamble
\microtypesetup{protrusion=false}
\maketitle
\microtypesetup{protrusion=true}

\chapter{Introduction}

Thesis: A CPS-like global transformation can compile the $\lambda$-calculus with
continuation marks into the plain $\lambda$-calculus in a semantics-preserving way.

Continuation marks \cite{clements2006portable} are a language feature that provides a
mechanism to annotate the evaluation context of a program. This feature allows
arbitrary keys to be associated with arbitrary values for the lifetime of an evaluation
context and the inquiry of these values for a given set of
keys. This is advantageous for programs that require dynamic information about a program
execution such as debuggers, profilers, and steppers because it allows them to be defined
at the same level as the language instead of some level below.

The continuation-passing style (CPS) transformation is actually a family of 
transformations designed to make certain analyses simpler. Every member of this 
family shares a common trait: their performance augments each function 
with an additional formal parameter, the \emph{continuation}, a 
functional representation of currently pending computation. Functions in CPS 
never explicitly return; instead, they call the continuation argument with their 
result. Because no function ever returns, function calls are the final act of the 
caller. Thus, all calls are tail calls.
%a function call represents a permanent surrender of control.
%function calls permanently surrender control represents  
The CPS transformation then simplifies programs by representing all control 
and data transfer uniformly and explicitly. In general, the ``spirit'' of the 
CPS transformation is to represent all transfers of control uniformly \cite{sabry1994formal}.

It is our interest to understand the essence of continuation marks--their behavior in the 
absense of other language features and implementation details. For this, we take the core 
of computation, the $\lambda$-calculus, and add facilities to manipulate continuation 
marks. These two together comprise a language which we term $\lambda_{cm}$. By expressing 
$\lambda_{cm}$ in terms of the plain $\lambda$-calculus, we uncover the meaning of 
continuation marks in a pure computational language. We arrive at this expression by 
performing a transformation in the spirit of CPS from $\lambda_{cm}$ to the $\lambda$-calculus 
verified to preserve the meaning of the source language.

\chapter{Continuation marks}

There are certain tools that are indispensable to some programmers that concern the
behavior of their programs: debuggers, profilers, steppers, etc. Without these tools,
these programmers cannot justify the adoption of a language, however compelling it might
otherwise be. Traditionally, these tools are developed at the same level as the 
language, privy to incidental implementation detail, precisely because that detail 
enables these tools to function. This is problematic for at least two reasons. First, 
it couples the implementation of the tool with the implementation of the language, which
increases the cost to port to other platforms. If users become dependent upon these tools,
it can stall the advancement of the language and the adoption of new language features.
Second and more critical, it makes these tools unsound. For instance, debuggers typically
examine programs which have been compiled without optimizations. In general, this means 
that the debugged program has different behavior than the deployed program. This is 
obviously undesirable.

It is desirable to implement such tools at the same level as the language, removing
dependency upon the implementation an instead relying on definitional and behavioral
invariants. Continuation marks are a language-level feature that provide the information
necessary for these tools to function. Furthermore, languages which require stack
inspection to enforce security policies (\emph{Java}, \emph{C\#}) or support aspect
oriented programming (\emph{aspectj}) can be defined in terms of a simpler language with
continuation marks.

Continuation marks originated in PLT Scheme (now Racket \cite{plt-tr1}) as a stack 
inspection mechanism. In fact, the \emph{Java} and \emph{C\#} languages rely on a similar 
stack inspection to enforce security policies of which continuation marks can be seen as 
a generalization. Surprisingly, continuation marks can be encoded in any language with 
exception facilities \cite{pettyjohn2005continuations} which fact has led to their 
experimental addition to Javascript \cite{clements2008implementing}.

The feature of continuation marks itself is accessible via two surface level syntactic
forms: \emph{with-continuation-mark} and \emph{current-continuation-marks}.

\emph{with-continuation-mark} has three parameters: a key expression \emph{key-expr}, a 
value expression \emph{value-expr} and a body expression \emph{body-expr}. The evaluation 
of \emph{value-expr} will be associated with a key, the evaluation of \emph{key-expr}, 
before the evaluation of \emph{body-expr}. During the lifetime of the evaluation of 
\emph{body-expr}, a continuation mark will exist associated with this key. In a Scheme-like 
syntax, this call appears like so:

% will use slatex for the dissertation; too much of a hassle now
\texttt{(with-continuation-mark} \emph{key-expr} \emph{value-expr} \emph{body-expr}\texttt{)}

\emph{current-continuation-marks} has one parameter, a set of keys \emph{key-set}, and returns a list of
all the associated values attached to the dynamic context of the invocation. If a particular 
context has been annotated by more than one key in the set, this will be reflected in the 
returned list\footnote{The returned list is typically a list of non-empty lists where each 
sub-list represents the marks on a context.}. Additionally, the order in which values were 
attached with a particular key is preserved. Scheme-like, \emph{current-continuation-marks}
looks like this:

\texttt{(current-continuation-marks} \emph{key-set}\texttt{)}

Importantly, the result of \emph{current-continuation-marks} provides no evidence of any portion of the
dynamic context lacking continuation marks with the specified keys. This preserves the
ability to perform optimizations without exposing details which would render the
optimizations unsound. This also requires special consideration of a language that
supports tail call optimization (which is not an optimization in the above sense since its
behavior is defined in the semantics of the language). By definition, \emph{body-expr} is 
in tail position; a language with tail call optimization will reflect this.

The canonical example to illustrate the behavior of continuation marks in the presence and
absence of proper tail recursion is the factorial function.

Figure \ref{fac-rec} illustrates the definitional recursive variant of the factorial
function. In this actualization, a cascade of multiplication operations builds as the
recursive calls are made. Each multiplication is computation that must be performed after
the recursive call of which the machine must keep track.

\begin{figure}
\begin{verbatim}
(define (fact n)
  (if (= n 0)
      1
      (* n (fact (- n 1)))))
\end{verbatim}
\caption{The definitionally recursive factorial function}
\label{fac-rec}
\end{figure}

Figure \ref{fac-tail-rec} illustrates the tail recursive manifestation of the factorial
function. In contrast to the function in figure \ref{fac-rec}, this formulation performs
the multiplication before the recursive call. Because the function has no pending
computations after the evaluation of the recursive call, the execution context need not
grow. Such a call is said to be in tail position.

\begin{figure}
\begin{verbatim}
(define (fact-tr n acc)
  (if (= n 0)
      acc
      (fact-tr (- n 1) (* n acc))))
\end{verbatim}
\caption{A tail-recursive variant of the factorial function}
\label{fac-tail-rec}
\end{figure}

Figures \ref{fac-rec-cm} and \ref{fac-tail-rec-cm} represent these two variants of the
factorial function augmented with continuation marks. Using these definitions, the 
result of \texttt{(fact 3)} would be

\begin{verbatim}
(((fact 1)) ((fact 2)) ((fact 3)))
6
\end{verbatim}

whereas the result of \texttt{(fact-tr 3 1)} would be

\begin{verbatim}
(((fact 1)))
6
\end{verbatim}

\begin{figure}
\begin{verbatim}
(define (fact n)
  (if (= n 0)
      (begin
        (display (current-continuation-marks '(fact)))
        1)
      (with-continuation-mark 'fact n (* n (fact (- n 1)))))
\end{verbatim}
\caption{The definitionally recursive factorial function augmented with continuation marks}
\label{fac-rec-cm}
\end{figure}

\begin{figure}
\begin{verbatim}
(define (fact-tr n acc)
  (if (= n 0)
      (begin
        (display (current-continuation-marks '(fact)))
        acc)
      (with-continuation-mark 'fact n (fact-tr (- n 1) (* n acc))))
\end{verbatim}
\caption{The tail-recursive factorial function augmented with continuation marks}
\label{fac-tail-rec-cm}
\end{figure}

This difference is due to the growing continuation in the definitionally recursive
\texttt{fact}. Each call to \texttt{fact} has a pending computation--namely, the
multiplication--after the recursive call and so each necessitates the creation an
additional evaluation context. The effect of these additional contexts is that each
annotation is applied to a new, ``blank'' context, so all the annotations are preserved. 
In the tail-recursive variant, there is no pending computation and therefore no additional
evaluation context. In this instance, the previous mark is overwritten with the new.

\chapter{CPS transformations}

The CPS transformation is a family of language transformations derived from Plotkin
\cite{plotkin1975call} and designed to simplify  programs by representing all data and
control flow uniformly and explicitly, in turn simplifying compiler construction and
analyses such as optimization and verification \cite{sabry1994formal}. The standard
variation adds a formal parameter to every function definition and an argument to every
call site.

As an example, consider once again the two variants of the factorial function, sans
continuation marks, given earlier. In CPS, the properly recursive variant can be expressed
as
\begin{verbatim}
(define (fact n k) 
  (if (= n 0)
      (k 1)
      (fact (- n 1) (lambda (acc) (k (* n acc))))))
\end{verbatim}
and the tail-recursive variant as
\begin{verbatim}
(define (fact-tr n acc k)
  (if (= n 0)
      (k acc)
      (fact-tr (- n 1) (* n acc) k)))
\end{verbatim}
(For clarity, we have treated ``primitive'' functions--equality comparison, subtraction,
and multiplication--in a direct manner. In contrast, a full CPS transformation would
affect \emph{every} function.)

Notice that, in the first variation, each recursive call receives a newly-constructed $k$
encapsulating additional work to be performed at the completion of the recursive
computation. In the second, $k$ is passed unmodified, so while computation occurs within
each context, no \emph{additional} computation pends. From this example, we see that the
CPS representation is ideal for understanding tail-call behavior as it is explicit that
the continuation is preserved by the tail call.

The purpose of CPS does not lie solely in pedagogy, however. The reification of and
consequent ability to directly manipulate the continuation is a powerful ability,
analogous in power to the ability to \emph{capture} a continuation which some languages
provide. In Scheme, this is accomplished with \emph{call/cc}, short for ``call with
current continuation''. This call takes one argument which itself is a function of one
argument. \emph{call/cc} calls its argument, passing in a functional representation of the
current continuation--the continuation present when \emph{call/cc} was invoked. This
continuation function takes one argument which is treated as the result of \emph{call/cc}
and runs this continuation to completion.

As a simple example,
\begin{verbatim}
(+ 1 (call/cc
       (lambda (k)
         (k 1))))
\end{verbatim}
returns $2$. In effect, invoking $k$ with the value 1 is the same as replacing the entire
\emph{call/cc} invocation with the value 1.

Much of the power of \emph{call/cc} lies in the manifestation of the continuation as a
function, giving it first-class status. It can be passed as an argument in function calls,
invoked, and, amazingly, reinvoked at leisure. It is this reinvokeability that makes
\emph{call/cc} the fundamental unit of control from which all other control structures can
be built, including generators, coroutines, and threads.

%\emph{call/cc} is erroneously seen as incredibly heavyweight and overkill for control
%(cite something). This conception probably comes from the conceptualization of the
%continuation as the call stack and continuation capture as stack copy while continuation
%call is stack installation. It is also seen as a form of \emph{goto} which is known to
%obfuscate control flow and impede analysis (cite something). [Can't and shouldn't talk
%much about the second. Probably take it out.] Bearing in mind that the CPS transformation
%aids compilers, it is useful to investigate the characterization of \emph{call/cc} within
%the standard CPS transformation.

In direct style, the definition of \emph{call/cc} is conceptually 
\begin{verbatim}
(define call/cc
  (lambda (f)
    (f (get-function-representing-continuation))))
\end{verbatim}
where \emph{get-function-representing-continuation} is an opaque function which leverages
sweeping knowledge of the language implementation. The CPS definition is notably easier:
\begin{verbatim}
(define call/cc
  (lambda (f k)
    (f k k)))
\end{verbatim}

Variations on the standard CPS transformation can make the expression of certain control
structures more straightforward. For instance, the ``double-barrelled'' CPS transformation
is a variation wherein each function signature receives not one but two additional formal
parameters, each a continuation. One application of this particular variation is error
handling with one continuation argument representing the remainder of a successful
computation and the other representing the failure contingency. It is especially useful in
modelling exceptions and other non-local transfers of control in situations where the
computation might fail. In general, the nature of the CPS transformation allows it to
untangle complicated, intricate control structures.

Similar transformations exist which express other programming language features such as
security annotations \cite{wallach2000safkasi} and control structures such as procedures,
exceptions, labelled jumps, coroutines, and backtracking. On top of other offerings, this
places it in a category of tools to describe and analyze programming language features.
(This category is also occupied by Moggi's computational lambda calculus--monads
\cite{moggi1989computational}.)

\subsection{Example CPS transform}

We will now focus our attention on a CPS transform defined over the $\lambda$-calculus.
Before doing so, we offer a brief primer on the relevant parts of the $\lambda$-calculus.

\subsubsection{$\lambda$-calculus}

The $\lambda$-calculus is a Turing-complete system of logic extensively used as a formal
system for expressing computation. Terms in the $\lambda$-calculus are defined
inductively. Terms take the form of variables $x$ drawn from an infinite set, abstractions
$\lambda x.M$ where $M$ is itself a $\lambda$-calculus term, and applications $M\,N$ where
$M$ and $N$ are $\lambda$-calculus terms.

Variables in the $\lambda$-calculus are either free or bound. In the term consisting of
the lone variable $x$, $x$ is free, meaning that its value is determined solely by its
environment. If we abstract $x$ to create $\lambda x.x$, $x$ is no longer free but bound.
Bound variables play a special part in the role of the $\lambda$-calculus as a computation
substrate. Terms with no free variables can be called closed, combinators, or programs.

To a first approximation, abstractions are functions. For instance, the identity function
can be expressed as $\lambda x.x$ where $x$ is \emph{any} variable. Thus, there are an
infinite number of ways to express the identity function: $\lambda x.x$, $\lambda y.y$,
$\lambda z.z$, etc. These terms are not syntactically equivalent, but they are the same
function. This fact gives rise to the notion of $\alpha$-equivalence which captures the
idea that arbitrary, consistent renaming does not change the essence of a term. One
subtlety here (which we will also encounter later) is \emph{variable capture} which can
occur when the new name of a variable is already present in the term. For instance, in the
term $\lambda x.\lambda y.x$, if each $x$ is renamed to $y$, we obtain the term $\lambda
y.\lambda y.y$, a different term. For this reason, we need to take special care when we
rename variables (which we will need to do regularly).

One of the ways abstractions approximate functions is that we can apply them to arguments.
This is signified simply by juxtaposition of function (or operator) and argument (or
operand). In the correct context, an application of the form $M\,N$ can be \emph{reduced}
in which the operator $M$ is applied to the operand $N$. For instace, the term $\lambda
x.x\,y$ reduces to $y$. We must take care when reducing a term such as $\lambda x.\lambda
y.x\,y$. If we reduce naively, we obtain $\lambda y.y$ which does not reflect the intended
meaning of the reduction--the argument $y$ has been captured by the abstraction and is no
longer independent of that abstraction. In order to avoid this, we can rename capturing
abstractions in $M$ to be outside the set of free variables of $N$. In $\lambda x.\lambda
y.x\,y$, we rename $y$ to $z$ in $\lambda x.\lambda y.x$ and obtain $\lambda x.\lambda
z.x$. The subsequent reduction of $\lambda x.\lambda z.x\,y$ to $\lambda z.y$ correctly
reflects the intended meaning of the original term.

In the $\lambda$-calculus, evaluation occurs during reduction, and reduction is merely
application. There is, however, yet more subtlety of which we must be aware, namely, in
which contexts we allow applications to be reduced. In the call-by-name
$\lambda$-calculus, reduction beneath abstraction is legal. Under this regime, both
$\lambda y.y\,x$ and $\lambda x.(\lambda y.y\,x)$ are reducible. In the call-by-value
$\lambda$-calculus, however, reduction beneath abstraction is illegal. Under this regime,
$\lambda y.y\,x$ is reducible; by contrast, $\lambda x.(\lambda y.y\,x)$ is not.  The
latter term, when applied to $z$ in a call-by-name regime, would reduce as
\begin{align*}
            &\lambda x.(\lambda y.y\, x)\,z\\
\rightarrow &\lambda x.x\,z\\
\rightarrow &z
\end{align*}
whereas that same application in a call-by-value regime would reduce as
\begin{align*}
            &\lambda x.(\lambda y.y\, x)\,z\\
\rightarrow &\lambda y.y\,z\\
\rightarrow &z
\end{align*}
Notice that, in the call-by-name reduction, application occurs before the arguments are
reduced. In the call-by-value reduction, the argument is fully reduced before the
abstraction is applied. Although both terms reduce to the same term in this example, this
distinction is not merely pedantic: terms may reduce definitively in some reduction
regimes and fail to reduce completely in others! Historically at least, the call-by-name
and call-by-value reduction regimes underlie the distinction between so-called lazy and
eager languages.

One final observation we should make about the $\lambda$-calculus is which terms denote
values. A value should be, in a sense, irreducible and that criterion disqualifies
applications from being considered as values. A value should remain indepedent of
environment, i.e., it should not merely be a placeholder for arbitrary values, and that
criterion disqualifies lone variables from being considered as values.\footnote{In actual
fact, a value is an irreducible $\lambda$-calculus term paired with an environment which
provides values for constituent free variables.} Thus, we shall consider abstractions to
be the sole form values can take in the $\lambda$-calculus, habituating ourselves to the
idea that functions are data.

\subsubsection{CPS transform}

A CPS transformation is a global syntactic transformation of language terms. Recall that
terms in the $\lambda$-calculus take the form of lone variables $x$,
$\lambda$-abstractions $\lambda x.M$, and applications $M\,N$ where $M$ and $N$ are
themselves $\lambda$-calculus terms. A comprehensive CPS transform definition need only
specify transformations for these three categories. As an example, consider Fischer's CPS
transform \cite{fischer1972lambda}:
\begin{align*}
\mathcal{F}[x]           &= \lambda k.k\,x\\
\mathcal{F}[\lambda x.M] &= \lambda k.k\,(\lambda x.\mathcal{F}[M])\\
\mathcal{F}[M\,N]        &= \lambda k.\mathcal{F}[M](\lambda m.\mathcal{F}[N]\,(\lambda n.(m\,n)\,k))
\end{align*}
Fischer's CPS transform abstracts each term in the $\lambda$-calculus: lone variables
wait on a continuation, abstractions receive a degree of indirection, and even
applications, the sole reduction facility of the $\lambda$-calculus, become abstractions.
In essence, terms become suspended in wait of a continuation argument. By priming a term
so-transformed with a continuation function--even as simple as the identity--we instigate
a cascade of computation.

In the case of lone variables and applications, this transformation ultimately has no
effect on the result of the computation. However, abstractions, the values of the
$\lambda$-calculus, are contaminated by the transform. For example, consider the
transformation of $\lambda x.x$
\begin{align*}
\mathcal{F}[\lambda x.x] &= \lambda k.k\,(\lambda x.\mathcal{F}[x])\\
                         &= \lambda k.(k\,\lambda x.\lambda k.(k\,x))
\end{align*}
If we apply this term to the identity function, it reduces as
\begin{align*}
            &\lambda k.(k \lambda x.\lambda k.(k\,x))\,\lambda y.y\\
\rightarrow &\lambda y.y\,\lambda x.\lambda k.(k\,x)\\
\rightarrow &\lambda x.\lambda k.(k\,x)
\end{align*}
The result of this reduction, in common with most others, contains residual terms from the
CPS transform. We will need to account for this particular property of the Fischer's CPS
transform.

One of the many reasons continuation-passing style is useful is that is allows evaluation
order to be controlled. In fact, Plotkin \cite{plotkin1975call} showed that it can be used
to simulate the call-by-value $\lambda$-calculus with the call-by-name $\lambda$-calculus
and vice versa. We can show that, in the call-by-value $\lambda$-calculus, the passed
continuation is applied only to fully-reduced terms. This can be done by verifying this
property for each case of the CPS transform definition. Before doing so, we remind that no
reduction is performed by effect of the transformation.
\[
T[x]=\lambda k.(k\,x)
\]
Variables are irreducible, so the property holds immediately here.
\[
T[\lambda x.E]=\lambda k.(k\,\lambda x.T[E])
\]
Here we reason inductively by assuming that the property holds for E. Abstractions are
irreducible in a call-by-value regime, so the property holds in this case also.
\[
T[E\,F]=\lambda k.(T[E]\,(\lambda e.(T[F]\,(\lambda f.((e\,f)\,k))))
\]
In this case, we observe that the continuation $k$ provided is not directly applied.
Instead, the operator is evaluated and a continuation is constructed into which its value
may be passed. The operand is treated similarly. Finally, the operator is applied to the
operand and $k$ is not invoked but passed to the result.

Notice that $k$, which represents the rest of the continuation, is passed unmodified.
Specifically, a new continuation which employs $k$ as a subcontinuation has not been
constructed. This quality is characteristic of a tail call in which the evaluation context
for the arguments, no longer necessary, is subsumed by the evaluation context of the newly
formed application.

Finally, we observe that every transformed application passes--rather than invokes--the
provided continuation and that only after reduction occurs. Then continuations can be
invoked only as in the cases of variables and abstractions and we can see that they are 
only applied to fully-reduced terms.

%Suppose we want to embed direct-style terms in CPS terms. We are interested in what cases we can induce reduction.

%As an example which will become useful later, suppose we want to deal with a Church encoding of lists directly.

%Recall that we can represent \emph{nil} as [the Church encoding of] \emph{false} and \emph{cons a b} as $\lambda p.((p a) b)$.

%Like [the transformation] T, C has a similar property which we quickly verify:

%\begin{align*}
%C[x]           &= \lambda k.\lambda m.(k x)\\
%C[\lambda x.E] &= \lambda k.\lambda m.(k \lambda x.C[E])\\
%C[E F]         &= \lambda k.\lambda m.((C[E] (\lambda e.((C[F] (\lambda f.(((e f) k) m))) g[m]))) g[m])\\
%C[wcm E F]     &= \lambda k.\lambda m.((C[E] (\lambda e.((C[F] k) h[e,m]))) g[m])\\
%C[ccm]         &= \lambda k.\lambda m.(k i[m])\\
%\end{align*}

%where g[m]=cons false (snd m)
%and h[e,m]=cons true (if fst m then rest (snd m) else snd m)
%and i[m]=snd m

%We use the definitions $g$, $h$, and $i$ to emphasize that these facilities enjoy privileged status in the transformation definition and are invisible to the program.

%-uses
%-lambda calculus
%-reference CPS due to Fischer/Plotkin
%-properties

\chapter{$\lambda_{cm}$}

We consider an extension of the call-by-value $\lambda$-calculus with facilities to
manipulate continuation marks, introduced by Pettyjohn et al.
\cite{pettyjohn2005continuations}, which we term $\lambda_{cm}$.

\section{Definition of $\lambda_{cm}$}

Figure \ref{language-syntax} presents the syntactic definition of $\lambda_{cm}$.
Definitions of $E$ and $F$ signify evaluation contexts. The non-terminal $E$ is simply any
term in the language but for the constraint that evaluation contexts interleave \emph{wcm}
directives. The definition of $e$ establishes the forms of valid expressions in the
language: applications, variables, values, \emph{wcm} forms, and \emph{ccm} forms. The
definition of $v$ denotes that values in the language are $\lambda$-abstractions. (Also,
notice that \emph{wcm} and \emph{ccm} have one parameter fewer than the forms introduced
earlier. This is because $\lambda_{cm}$ expresses unkeyed marks. While it can be shown
that a language with keyed marks can be expressed in terms of a language with unkeyed
marks, such as $\lambda_{cm}$, this is not our concern.)

\begin{figure}
\begin{align*}
E = &(\mathrm{wcm}\,v\,F) & e = &(e\,e)\\
    &F                    &     &x\\
F = &[]                   &     &v\\
    &(E\,e)               &     &(\mathrm{wcm}\,e\,e)\\
    &(v\,E)               &     &(\mathrm{ccm})\\
    &(\mathrm{wcm}\,E\,e) & v = & \lambda x. e
\end{align*}
\caption{$\lambda_{cm}$ syntax}
\label{language-syntax}
\end{figure}

Figure \ref{language-semantics} presents the semantics of $\lambda_{cm}$. The definitions
therein establish the proper interpretation of various expressions. The first follows the
typical definition of application. The second defines the tail behavior of the \emph{wcm}
form. The third expresses that the \emph{wcm} form takes on the value of its body.
Finally, the fourth defines the value of the \emph{ccm} form in terms of the $\chi$
metafunction which definition is given in figure \ref{chi-metafunction}. The $\chi$
metafunction is defined over syntactic forms of the language, so its definition
corresponds closely to the definition of $\lambda_{cm}$ found in figure
\ref{language-syntax}.

\begin{figure}
\begin{align*}
E[(\lambda x.e)\,v]                         &\rightarrow E[e[x\leftarrow v]]\\
E[(\mathrm{wcm}\,v\,(\mathrm{wcm}\,v'\,e))] &\rightarrow E[(\mathrm{wcm}\,v'\,e)]\\
E[(\mathrm{wcm}\,v\,v')]                    &\rightarrow E[v']\\
E[(\mathrm{ccm})]                           &\rightarrow E[\chi(E)]
\end{align*}
\caption{$\lambda_{cm}$ evaluation rules}
\label{language-semantics}
\end{figure}

\begin{figure}
\begin{align*}
\chi([])                   &= \mathrm{empty}\\
\chi((E\,e))               &= \chi(E)\\
\chi((v\,E))               &= \chi(E)\\
\chi((\mathrm{wcm}\,E\,e)) &= \chi(E)\\
\chi((\mathrm{wcm}\,v\,E)) &= v : \chi(E)
\end{align*}
\caption{Definition of $\chi$ metafunction}
\label{chi-metafunction}
\end{figure}

\section{A Redex interpreter for $\lambda_{cm}$}

Redex \cite{findler2010redex} is a domain-specific language for exploring language
semantics. It lives very close to the semantics notation we have used in this discussion.
To illustrate how easily langagues can be defined in Redex, we will examine a Redex
program which defines a toy language. In contrast to a Redex tutorial, we will not concern
ourselves with the syntax and structure of roads not taken and will instead briefly
explain each component of the program.

\begin{verbatim}
(define-language toy
  (x variable-not-otherwise-mentioned)
  (v number undefined) 
  (e (+ e e) (with (x e) e) x v)
  (E hole (+ E e) (+ v E) (with (x E) e)))
\end{verbatim}

This expression defines the abstract syntactic structure of a language named \emph{toy}.
There are four categories of structures: $x$, $v$, $e$, and $E$. The category $x$ is
defined to contain any token not otherwise mentioned in the definition. The category $v$
is defined to contain numbers and the token \emph{undefined}. The category $e$ is defined
to contain the expression forms of the language, of which there are four: addition
expressions, \emph{with} expressions, lone variables, and lone values. The last category,
$E$, does not define abstract syntax but instead reduction contexts. The first reduction
context is a \emph{hole} (a special token in Redex) which will be filled in with the
result of the expression that previously resided in its place. The next two are addition
contexts, the first representing the evaluation of the first argument and the second
representing the evaluation of the second; the composition of these contexts imposes an
order on the evaluation of the arguments. The final context, a \emph{with} context,
specifies a variable, a value, and an expression within which that variable is bound to
that value.

\begin{verbatim}
(define toy-rr
  (reduction-relation
   toy
   (--> (in-hole E (+ number_1 number_2))
        (in-hole E ,(+ (term number_1) (term number_2)))
        "+")
   (--> (in-hole E (with (x_1 v_1) e_1))
        (in-hole E (substitute x_1 v_1 e_1))
        "with")
   (--> (in-hole E x_1)
        (in-hole E undefined)
        "free variable")
   (--> (in-hole E (+ undefined e_1))
        (in-hole E undefined)
        "undefined in first position")
   (--> (in-hole E (+ number_1 undefined))
        (in-hole E undefined)
        "undefined in second position")))
\end{verbatim}

This term defines a reduction relation on the \emph{toy} language. The five defined
reductions, signalled by \texttt{-->}, match specified patterns and manipulate them
according to the defined rules. These define: the addition of two numbers; the
substitution of a \emph{with} expression; a lone variable; the addition of an undefined
value on the left; and the addition of an undefined value on the right.

\begin{verbatim}
(define-metafunction toy
  substitute : x v e -> e
  [(substitute x_1 v_1 (+ e_1 e_2))
   (+ (substitute x_1 v_1 e_1) (substitute x_1 v_1 e_2))]
  [(substitute x_1 v_1 (with (x_1 e_1) e_2))
   (with (x_1 (substitute x_1 v_1 e_1)) e_2)]
  [(substitute x_1 v_1 (with (x_2 e_1) e_2))
   (with (x_2 (substitute x_1 v_1 e_1)) (substitute x_1 v_1 e_2))]
  [(substitute x_1 v_1 x_1)
   v_1]
  [(substitute x_1 v_1 x_2)
   x_2]
  [(substitute x_1 v_1 v_2)
   v_2])
\end{verbatim}

The definition of the \emph{with} reduction rule relies on the \emph{substitute}
metafunction. (Metafunctions assist in defining the language, and therefore exist outside
of that language; hence, they are not functions of the language but metafunctions.) The
\emph{substitute} metafunction recursively substitutes a variable in an expression with a 
value. The substitution is only propagated as long as a binding with the same name is not 
encountered. At that point, the substitution is performed in the value expression of the 
binding, but not the body. This allows for expressions like
\begin{verbatim}
(with (x 5)
  (with (x x)
    x))
\end{verbatim}
to behave as we expect (returning 5).

Now that the syntactic forms and reduction rules of the language are defined, we can use 
the randomized testing built into Redex to investigate properties of the language. We 
start by defining the helper function \emph{reduces-to-one-value?} (which has its own 
helper function \emph{value?}).
\begin{verbatim}
(define value? (redex-match toy v))

(define (reduces-to-one-value? e)
  (let ((results (apply-reduction-relation* toy-rr e)))
    (and (= (length results) 1)
         (value? (first results)))))
\end{verbatim}
The \emph{*} at the end of the function name \emph{apply-reduction-relation*} signifies
that all possible reduction rules will be applied as many times as possible. If some of
the reduction rules don't actually reduce terms, the relation may produce a reducible term
indefinitely. The function \emph{apply-reduction-relation*} is in a sense strict in the
reduction relation and will likewise run indefinitely if this is the case.

After the language and some properties have been established, the randomized testing, 
initiated by
\begin{verbatim}
(redex-check toy
             e
             (reduces-to-one-value? (term e)))
\end{verbatim}
is simple. We merely provide the name of the language we wish to work with, the
nonterminal in the grammar we wish to use to generate language terms, and a predicate that
checks terms for properties. This function generates terms gradually increasing in size,
applying the predicate to each in turn, and terminates with a counterexample or after a
set number of terms have been checked (1000 by default).

Randomized testing can increase our confidence in various assertions but is no substitute 
for proof. We express the property of reducing to one value with the following theorem:

%(e (+ e e) (with (x e) e) x v)
\newtheorem*{toythm}{Toy Language One-Value Theorem}
\begin{toythm}
For all terms $e$ of the toy language, $e$ reduces to exactly one value.\footnote{We do 
not use the term ``value'' loosely here; the toy language definition specifies what 
constitute values, and we appeal to this.}
\end{toythm}
We proceed by induction on the structure of terms $e$ of the toy language. First, we 
consider the base cases.
\begin{proof}[Case $v$]
A term $e$ of the form $v$ is exactly one value and cannot be reduced, so the statement 
holds.
\end{proof}
\begin{proof}[Case $x$]
A term $e$ of the form $x$, a variable, reduces to \emph{undefined}, a value term, so the 
statement holds.
\end{proof}
\begin{proof}[Case $(with\,(x\,e_{1})\,e_{2})$]
By induction, we assume that $e_{1}$ reduces to exactly one value. Then the ``with''-rule
can only be applied once, resulting in a single term $e_{2}$ in $e$ which, by our
inductive hypothesis, reduces to only one value.
\end{proof}
\begin{proof}[Case $(+\,e_{1}\,e_{2})$]
By induction, we assume both $e_{1}$ and $e_{2}$ reduce to a single value. We consider two
subcases: If $e_{1}$ reduces to \emph{undefined}, the ``undefined in first position''-rule
is applied, and the whole term reduces to \emph{undefined}. If $e_{1}$ reduces to a
number, we consider two further subcases: If $e_{2}$ reduces to a number, the ``+''-rule
is applied, and the entire expression reduces to the sum of the two numbers obtained. If
$e_{2}$ reduces to \emph{undefined}, the ``undefined in second position''-rule is applied,
and the entire term reduces to \emph{undefined}. Thus, in all subcases, the whole term 
reduces to exactly one value.
\end{proof}

This property is fairly trivial and its proof is similarly trivial, but it is a shadow of 
the approach we will ultimately take to verify our transform preserves meaning.

\section{Language translation}

$\lambda_{cm}$ is a strict superset of $\lambda_{v}$; there are within $\lambda_{cm}$
syntactic forms inscrutable to a $\lambda_{v}$ interpreter. Additionally, evaluation order
within these forms is significant and a naive treatment of these forms by $\lambda_{v}$
will most likely be incorrect (which is to say, we cannot simply inherit its reduction
order). Plotkin \cite{plotkin1975call} demonstrated that the CPS transform can be used to
control evaluation order. We will use this facility in our transformation to express terms
in $\lambda_{cm}$ in terms of $\lambda_{v}$. [Other analysis which the CPS transformation
affords us \cite{appel2007compiling} is incidental.]

The work of the transformation then is to express the five syntactic forms of
$\lambda_{cm}$ ($e$-terms in figure \ref{language-syntax}) in the three syntactic forms of
$\lambda_{v}$ (the first three $e$-terms of that same figure) in a semantics-preserving
way. Much of the effort of our analysis and justification will be in demonstrating that
the semantics are indeed preserved, namely the tail-call behavior.

In continuation-passing style, every term takes an additional argument, the continuation,
into which the evaluation is passed. Our approach to a semantics-preserving
transformation, which we call $\mathcal{C}$, takes this a step further. Intuitively, we
will pass not only a continuation but also a list of the current continuation marks. The
\emph{wcm} form will add a mark to this list and the \emph{ccm} form will simply return it.

\subsection{First pass toward $\mathcal{C}$}

Let us denote a first pass toward $\mathcal{C}$ as $\mathcal{C}_{1}$. We define 
$\mathcal{C}_{1}$ formally as 
\begin{align*}
\mathcal{C}_{1}[x]           &= \lambda k.\lambda m.(k\,x)\\
\mathcal{C}_{1}[\lambda x.E] &= \lambda k.\lambda m.(k\,\lambda x.\mathcal{C}_{1}[E])\\
\mathcal{C}_{1}[E\,F]        &= \lambda k.\lambda m.((\mathcal{C}_{1}[E]\,(\lambda e.((\mathcal{C}_{1}[F]\,(\lambda n.(((e\,f)\,k)\,m)))\,m)))\,m)\\
\mathcal{C}_{1}[wcm\,E\,F]   &= \lambda k.\lambda m.((\mathcal{C}_{1}[E]\,(\lambda e.((\mathcal{C}_{1}[F]\,k)\,\lambda p.((p\,e)\,m))))\,m)\\
\mathcal{C}_{1}[ccm]         &= \lambda k.\lambda m.(k\,m)
\end{align*}

Despite its gross deficiencies, this definition captures some of the critical aspects of a
correct transform, namely, the explicit ordering of evaluation in the \emph{wcm} form and
the conceptual simplicity of the \emph{ccm} form by virtue of our continuation
mark-passing approach.

We notice, however, that the list of marks retrieved by \emph{ccm} is ordered from the
innermost context outward whereas the semantics of $\lambda_{cm}$ specify the reverse
ordering. Typically, our approach would be to correct our transform to follow the semantic
definition. We would probably do this by reversing the order of the marks within
\emph{ccm}. However, the mark order is an inconsequential part of the definition. Rather
than needlessly clutter our definition with manipulation, we will opt instead to modify
our target semantics by modifying the $\chi$-metafunction, as we did in our Redex
interpreter. We realize this is a slippery slope and promise this is the only time we'll
do it.

\subsection{What do you mean ``equivalent''?}

At this point, it may be profitable to discuss the properties of a correct transform from
$\lambda_{cm}$ to $\lambda_{v}$. Because $\lambda_{cm}$ is a superset of $\lambda_{v}$, we
should expect terms of $\lambda_{cm}$ that are also in $\lambda_{v}$ to be equivalent in
some sense. What sense? Consider the term
\[
\lambda x.x
\]
in $\lambda_{cm}$. As an abstraction, this term denotes a value and would expect, at very
least, it to be transformed into a value in $\lambda_{v}$. However, we need to keep in
mind that in continuation-passing style, \emph{all} terms are abstractions, and therefore
values. For, consider the term
\[
\lambda x.x\,\lambda y.y
\]
also in $\lambda_{cm}$. Fischer's CPS transformation of this is
\begin{align*}
            &\mathcal{F}[\lambda x.x\,\lambda y.y]\\
= &\lambda k.(\mathcal{F}[\lambda x.x]\,(\lambda e.\mathcal{F}[\lambda y.y]\,(\lambda f.((e\,f)\,k))))\\
= &\lambda k.(\lambda k.(k\,\lambda x.\mathcal{F}[x])\,(\lambda e.\mathcal{F}[\lambda y.y]\,(\lambda f.((e\,f)\,k))))\\
= &\lambda k.(\lambda k.(k\,\lambda x.\lambda k.(k\,x))\,(\lambda e.\mathcal{F}[\lambda y.y]\,(\lambda f.((e\,f)\,k))))\\
= &\lambda k.(\lambda k.(k\,\lambda x.\lambda k.(k\,x))\,(\lambda e.\lambda k.(k\,\lambda y.\mathcal{F}[y])\,(\lambda f.((e\,f)\,k))))\\
= &\lambda k.(\lambda k.(k\,\lambda x.\lambda k.(k\,x))\,(\lambda e.\lambda k.(k\,\lambda y.\lambda k.(k\,y))\,(\lambda f.((e\,f)\,k))))
\end{align*}
which, strictly speaking, produces a value. What we're really interested in is the form of 
the result of the computation and, despite the radical change that has taken place, no 
computation has been performed. Instead, the transformation produces a term that awaits 
a continuation--any continuation--to instigate evaluation. At the top level, we are simply 
interested in seeing the result so we apply this term to simply the identity function 
$\lambda z.z$. Reduction proceeds as
\begin{align*}
            &\lambda k.(\lambda k.(k\,\lambda x.\lambda k.(k\,x))\,(\lambda e.\lambda k.(k\,\lambda y.\lambda k.(k\,y))\,(\lambda f.((e\,f)\,k))))\,\lambda z.z\\
\rightarrow_{\lambda_{v}} &\lambda k.(k\,\lambda x.\lambda k.(k\,x))\,(\lambda e.\lambda k.(k\,\lambda y.\lambda k.(k\,y))\,(\lambda f.((e\,f)\,\lambda z.z)))\\
\rightarrow_{\lambda_{v}} &\lambda e.\lambda k.(k\,\lambda y.\lambda k.(k\,y))\,(\lambda f.((e\,f)\,\lambda z.z))\,\lambda x.\lambda k.(k\,x)\\
\rightarrow_{\lambda_{v}} &\lambda k.(k\,\lambda y.\lambda k.(k\,y))\,(\lambda f.((\lambda x.\lambda k.(k\,x)\,f)\,\lambda z.z))\\
\rightarrow_{\lambda_{v}} &\lambda f.((\lambda x.\lambda k.(k\,x)\,f)\,\lambda z.z)\,\lambda y.\lambda k.(k\,y)\\
\rightarrow_{\lambda_{v}} &(\lambda x.\lambda k.(k\,x)\,\lambda y.\lambda k.(k\,y))\,\lambda z.z\\
\rightarrow_{\lambda_{v}} &\lambda k.(k\,\lambda y.\lambda k.(k\,y))\,\lambda z.z\\
\rightarrow_{\lambda_{v}} &\lambda z.z\,\lambda y.\lambda k.(k\,y)\\
\rightarrow_{\lambda_{v}} &\lambda y.\lambda k.(k\,y)
\end{align*}
We contrast this with the result of the direct-style evaluation, $\lambda y.y$, and notice 
that the CPS transformation, both global and thorough, has contaminated even the value our 
program produces. This observation will influence both the exact properties we should 
expect a correct transform should have and our construction of it.


The transformation $\mathcal{C}_{1}$ is fundamentally similar to a CPS transformation and 
so shares the attribute that transformed terms must be ``primed'' with a continuation in 
order for evaluation to proceed. In fact, terms produced by $\mathcal{C}_{1}$ are doubly 
abstracted, also requiring an argument encoding the current continuation marks. For now, 
we will assume that this approach is a fine way to go.

[say something here about...]

It is at this point that we notice that, however we choose to encode it, the empty list 
should initiate the program. Having established this, we can state some general facts about 
$\mathcal{C}$:

\begin{enumerate}
\item Like other CPS transformations, it contaminates values.
\end{enumerate}

Consider the program \emph{ccm}. In $\lambda_{cm}$, it evaluates to the empty list under 
some encoding; that is,
\[
\mathrm{ccm}\rightarrow_{\lambda_{cm}}empty
\]
We also have
\[
\mathcal{C}_{1}[\mathrm{ccm}]=\lambda k.\lambda m.(k\,m)
\]
and assuming for now that the identity function is a satisfactory continuation to provide 
and letting $[]$ represent our encoding of the empty list, we have
\begin{align*}
            &(\lambda k.\lambda m.(k\,m)\,\lambda z.z)\,[]\\
\rightarrow_{\lambda_{v}} &\lambda m.(\lambda z.z\,m)\,[]\\
\rightarrow_{\lambda_{v}} &\lambda z.z\,[]\\
\rightarrow_{\lambda_{v}} &[]
\end{align*}
Here we immediately observe that \emph{empty} and $[]$ should be equivalent in some
sense, and can concretely provide that sense:
\[
E\rightarrow_{\lambda_{cm}}v'\Rightarrow(\mathcal{C}[E]\,\lambda x.x)\,\mathcal{C}_{v}[\mathrm{empty}]\rightarrow_{\lambda_{v}}\mathcal{C}_{v}[v']
\]
That is, we expect that a transformed program should yield a value transformed by a
similar but not necessarily identical $\mathcal{C}$ which we call $\mathcal{C}_{v}$.


We now turn our attention to the tail-call behavior of $\mathcal{C}_{1}$. According to the 
semantics of $\lambda_{cm}$, the program 
$(\mathrm{wcm}\,a\,(\mathrm{wcm}\,b\,(\mathrm{ccm})))$ evaluates to b:
$(\mathrm{wcm} a (\lambda x.x (\mathrm{wcm}\,b\,(\mathrm{ccm}))))$. 

---

Before we prove the theorem generally, let's examine a few cases and investigate why it is 
true.

First, consider any program devoid of \emph{wcm} and \emph{ccm} forms. As $\lambda_{cm}$ is 
a superset of the $\lambda$-calculus, such a program considered in $\lambda_{cm}$ should 
reduce to an equivalent value as when considered in the $\lambda$-calculus. More precisely, 
if $e$ is a program,
\[
e\rightarrow_{\lambda_{cm}}v
\]
and
\[
e\rightarrow_{\lambda_{v}}v'
\]
then $v=v'$. 

This case is complicated by our transform, however, as terms in $\lambda_{cm}$ are not 
directly interpreted as terms in the $\lambda$-calculus. The idea we are really after is, 
if $e$ is a program,
\[
e\rightarrow_{\lambda_{cm}}v
\]
and
\[
\mathcal{C}[e]\rightarrow_{\lambda_{v}}v'
\]
then $v\equiv v'$. We make a weaker assertion here that the terms are equivalent--not 
equal--and, by this, expect only that we can formally relate them.

Next, consider the program $ccm$, the smallest program strictly in $\lambda_{cm}$. 
The entire evaluation of this program exhibits only a single redex, a hole, and a $ccm$ 
term which fills it. According to the semantics of $\lambda_{cm}$ and further the definition of the 
$\chi$-metafunction, the result of this program is the empty list.

According to the definition of $\mathcal{C}$, the $ccm$ term is transformed to
\[
\lambda k.\lambda m.(\lambda k.(k\,(m\,\lambda x.\lambda y.y))\,\lambda m'.((m'\,k)\,z))
\]
and the outermost redex (in this case, the sole redex) is applied first to the continuation
\[
\lambda m'.\lambda k.\lambda m.(k\,m')
\]
and then to the marks argument
\[
\lambda p.((p\,\lambda x.\lambda y.y)\,\lambda k.\lambda m.(k\,\lambda x.\lambda k.\lambda m.(k\,\lambda y.\lambda k.\lambda m.(k\,y))))
\]

We certainly could reduce this by steps and see it work, but that would give us no intuition as to \emph{why} it works. Recall that our transform, in CPS spirit, augments each call site with two additional parameters, the second being an encoding of the tail position of the context and the continuation marks. The transform of $ccm$ discards the tail-position encoding and applies the supplied continuation to the encoding of the continuation marks. However, as each call site also requires an argument encoding the marks, a dummy variable is passed. This is sound as the marks parameter does not bind any variable in the body, so the dummy variable is promptly ignored. In this process, the double abstraction surrounding the list has been stripped away and should be restored. This is accomplished by the passed continuation which does exactly that. Since the result of the reduction is known to be irreducible--that is, either a variable or an abstraction--and since irreducible terms are wrapped in the same way, we can wrap the value manually, yielding a transformed value as if directly.

%---
%ok

%terms are nested in this language
%in other words, terms are built up from primitives or smaller terms
%then proof that the transform works amounts to:

%if the term is x and the marks argument (with flag) is y and the continuation k
%then the result is ...

%example:

%term is ccm, marks argument is (false,[]) and continuation is
 
\begin{align*}
C[x]                  &= \lambda k.\lambda m.(k\,x)\\
C[\lambda x.E]        &= \lambda k.\lambda m.(k\,\lambda x.C[E])\\
C[E\,F]               &= \lambda k.\lambda m.(\lambda k_{1}.(k_{1}\,(m\,\lambda x.\lambda y.y))\,\lambda m'.(\lambda k_{2}.(k_{2}\,\lambda p.((p\,\lambda x.\lambda y.y)\,m'))\,\lambda m''.((C[E]\,\lambda e.((C[F]\,\lambda f.(((e\,f)\,k)\,m))\,m''))\,m'')\\
C[\mathrm{wcm}\,E\,F] &= \lambda k.\lambda m.(\lambda k_{1}.(k_{1}\,(m\,\lambda x.\lambda y.y))\,\lambda m'.(\lambda \\
C[\mathrm{ccm}]       &= \lambda k.\lambda m.(((m\,\lambda x.\lambda y.y)\,k)\,x
\end{align*}

%For all contexts $E$, expressions $e$, marks $m$, and continuations $k$

%for any context $E$, marks $m$, and continuation $k$, \emph{ccm} applies the continuation $k$ to the applied value of the marks $m$.
%By definition, it strips away the flag and passes the continuation $k$ and a dummy variable $x$.
%Our inductive hypothesis is that the marks are of the form $\lambda k.\lambda m.(k\,E)$ where $E$ is some term.
%This holds if the marks are empty: the empty list is $\lambda k.\lambda m.(k \lambda x.\lambda k.\lambda m.(k \lambda y.\lambda k.\lambda m.(k y)))$
%and this is of the required form which can be seen by taking $E=\lambda x.\lambda...$
%If the marks are not the empty list, they are an encoded pair, of the form $\lambda k.\lambda m.(k\,\lambda p...)$

%preconditions for each form:
%abstracted to receive a continuation k and marks encoding m

%postconditions for each form:
%returns a "stripped" value

%show that the preconditions lead to the postconditions

%We must establish a few invariants.

%First, every evaluation will have a continuation and marks state passed to it. 
%Second, every continuation will be called with a "stripped" term--that is, 

\newtheorem{thm}{Theorem}
\newtheorem{case}{Case}
\begin{thm}
Every evaluation is passed a continuation and an encoding of the tail state and the marks.
\end{thm}
In a continuation-passing style, which we have adopted, the evaluation which occurs by application does so in two stages. First, the arguments of the application, the operator and operand, are evaluated. Second, the application itself is performed which effects the evaluation of the result.
The evaluation which occurs within a \emph{wcm} form also occurs in two stages, and continuation-passing style is used to control these stages.

%Application, \emph{wcm}, \emph{ccm}
\begin{proof}
\begin{case}
$E\,F$
\end{case}
\begin{proof}
proof of application
\end{proof}

\begin{case}
$\mathrm{wcm}\,E\,F$
\end{case}
\begin{proof}
stuff
\end{proof}

\begin{case}
$\mathrm{ccm}$
\end{case}
\begin{proof}
stuff
\end{proof}
This completes the proof.
\end{proof}

\begin{thm}
Every continuation is applied to a stripped term.
\end{thm}
%a term without a leading continuation and marks state parameterization but otherwise identical to a transformed term.

\begin{proof}[Case $x$]
stuff
\end{proof}
\begin{proof}[Case $\lambda x.E$]
stuff
\end{proof}
\begin{proof}[Case $E\,F$]
stuff
\end{proof}
\begin{proof}[Case $\mathrm{wcm}\,E\,F$]
stuff
\end{proof}
\begin{proof}[Case $\mathrm{ccm}$]
stuff
\end{proof}

%e=e e
%  wcm e e
%  ccm
%  v

%E=wcm v F
%F=wcm E e
%  E e
%  v E
%  hole
  
%abs x.E F -> E[x:=F]
%wcm v (wcm v' E) -> wcm v' E
%wcm v v' -> v'
%E[ccm]=chi[E]

%wcm v [

  
%hole[e e]
%hole[wcm e e]
%hole[ccm]
%hole[v]


%if the context is empty (meaning in this case hole?), it is false because that is 
%passed in

%show that wcm v (wcm e e') evaluates as it should
%wcm v (ccm)
%wcm v (e e')
%wcm v v'

%for wcm v (wcm e e')
%suppose we have wcm e (wcm e' e'')
%the transform reduces the first value, respecting the tail flag
%(wcm v (wcm e' e'')
%the transform performs a tail call passing the mark (prove if marks were correct, they are correct) stuff
%(wcm e' e'') (true,[v])
%the transform reduces the first value, respecting the tail flag

\newtheorem{case_tpf}{Case}
\begin{thm}
The tail position flag is correct meaning it is true if and only if the context $E=\mathrm{wcm}\,v\,F$.
\end{thm}
\begin{proof}
\begin{case_tpf}
$E=\mathrm{hole}$ %The beginning of evaluation (hole, here?) passes false, which is correct.
\begin{proof}
By definition, the initial continuation marks tail position flag is false.
\end{proof}
\end{case_tpf}
\begin{case_tpf}
$E=(E\,e)$
\begin{proof}
proof here
\end{proof}
\end{case_tpf}
\begin{case_tpf}
$E=(v\,E)$
\begin{proof}
proof here
\end{proof}
\end{case_tpf}
\begin{case_tpf}
$E=(\mathrm{wcm}\,E\,e)$
\begin{proof}
proof here
\end{proof}
\end{case_tpf}
\begin{case_tpf}
$E=(\mathrm{wcm}\,e\,F)$
\begin{proof}
proof here
\end{proof}
\end{case_tpf}
%E e passes false
%v E passes false
%wcm E e passes false
%wcm v F passes true
\end{proof}

%The tail marks list is correct.
%Proof
%The beginning of evaluation (hole, once again?) is the empty list, which is correct.
%E e passes list unmodified.
%v E passes list unmodified.
%wcm E e passes list unmodified.
%wcm v F overwrites if it is a tail call of another wcm, appends if not. correct either way.
%Now we can proceed with for all contexts, for all expressions, etc. (Main theorem)

%show behavior for ccm is correct for all contexts
%show behavior for (e e) is correct for all contexts (thus showing abs x E F rule?)
%show behavior for v is correct for all contexts (thus showing (wcm v v')?)
%show behavior for wcm v (wcm e e) is correct (thus showing (wcm v (wcm v' e)) -> wcm v' e) or (wcm v v') -> v'?
%show behavior for wcm (wcm e e) e is correct
%show behavior for hole (wcm e e) is correct
%show behavior for (wcm e e) e is correct
%show behavior for e (wcm e e) is correct


%show that only values get put in the list?
%that's a result of the call-by-value CPS

\newtheorem*{maintheorem}{The awesomeness of $\mathcal{C}$}
\begin{maintheorem}
For all contexts $E$, expressions $e$, continuations $k$ and marks encodings $m$, $E[e]\rightarrow_{\lambda_{cm}}v\Rightarrow \mathcal{C}[E][\mathcal{C}[e]]\rightarrow_{\lambda_{v}}\mathcal{C}[v]$.
\end{maintheorem}

%%%%%

%->_lv = t (\ans.\k.\m.k ans) (#f,nil)

%E[e] = x
%E = *, e = x
%C[E[e]] = \k.\m.k x
%v = x
%C[x] = \k.\m.k x
%v^ = \k.\m.k x

%(\k.\m.k x) (\ans.\k.\m.k ans) (#f,nil)
%(\ans.\k.\m.k ans) x
%(\k.\m.k x)

%E = * e_1, e = x
%C[E[e]] = ( (k1387)
%   ( (m1388)
 %    (( (k1387) (k1387 (m1388 ( (x) ( (y) y)))))
 %     ( (snd_m1391)
  %      (( (k1387) (k1387 ( (p) ((p ( (x) ( (y) y))) snd_m1391))))
   %      ( (m1388)
    %       ((( (k1383) ( (m1384) (k1383 x)))
     %        ( (e1389)
      %         ((( (k1385) ( (m1386) (k1385 e_1))) ( (f1390) (((e1389 f1390) k1387) m1388)))
       %         m1388)))
        %    m1388)))))))
%e_1 ->* v_1
%v = x v_1



%%%%%
We need to show that for /every/ evaluation context (i.e. continuation), we get the same
answer.

E=*, e=(ccm)
E[e]=(ccm)
C[E[e]]=\k.\m.(((m \x.\y.y) k) \z.z)
v=\x.\y.y
C[v]=\k.\m.(k \x.\k.\m.(k \y.\k.\m.(k y)))

((\k.\m.(((m \x.\y.y) k) \z.z) \v.\k.\m.(k v)) (#f,nil))
(\m.(((m \x.\y.y) \v.\k.\m.(k v)) \z.z) (#f,nil))
((((#f,nil) \x.\y.y) \v.\k.\m.(k v)) \z.z)

E=*, e=wcm e_1 e_2

C



\begin{proof}[Case $e=E\,F$]

\end{proof}

\begin{proof}[Case $E=\mathrm{wcm}\,hole e$, $redex=wcm\,E'\,F'$]
evaluate v (pretend it's not a value)
attach to marks according to outer context

\end{proof}

\begin{proof}[Case $E=\mathrm{wcm}\,v hole$, $e=wcm\,E'\,F'$]
evaluate v (pretend it's not a value)
attach to marks according to outer context (flag)
pass continuation unmodified and (true,marks) to F'

%\x.M N -> M[x:=N]
%(wcm v (wcm v' E)) -> (wcm v' E)
%(wcm v v') -> v'
%(ccm) -> chi(E)

%(wcm E (wcm E' (wcm E'' F)))
%(wcm v (wcm E' (wcm E'' F))) evaluate mark, append to list, mark flag true
%(wcm v (wcm v' (wcm E'' F))) evaluate mark
%(wcm v' (wcm E'' F)) replace in list, mark flag true
%(wcm v' (wcm v'' F)) evaluate mark
%(wcm v'' F) replace in list, mark flag true
\end{proof}

If we're dealing with the outermost context, the continuation is $\lambda x.\lambda k.\lambda m.(k x)$.

\begin{proof}[something]
By theorem 2, the continuation $k$ is always applied to a "stripped" value. The supplied continuation serves to unstrip it. Thus, the end result of the computation is identical to a transformed value.
\end{proof}

so, why does (wcm 1 (wcm 2 (ccm)) evaluate as it should?

1 is evaluated and attached to the list according to the tail flag
the continuation is passed unmodified indicating a tail call
thus, evaluation proceeds like this

(wcm 1 (wcm 2 (ccm))) (false,[])
(wcm 2 (ccm)) (true,[1])
(ccm) (true,[2])
[2]

provide an interpreter or new semantics for lambdacm that has the list of continuation 
marks on the side. the semantics of this are

%\x.E F (_,vs) -> E[x:=F] (false,vs)
%(wcm v E) (false,vs) -> E (true,v:vs)
%(wcm v E) (true,v':vs) -> E (true,v:vs)
%(ccm) (_,vs) -> vs

how do we show that these semantics are the same as the given?
also, since we're throwing around semantics, cite the source of the given semantics

call this reduction relation b where the other is a
then the property we want to show is

\[
E\rightarrow_{a}v\Rightarrow(E,(false,[]))\rightarrow_{b}v
\]

this won't work entirely because there is not a stack of states
consider (wcm E e)

%so (wcm E e) (_,vs) -> E (false,vs) -> (wcm v E) (_,vs)

%the rule is, for
%(wcm e f) (false,vs)

%if e (false,vs) -> v
%(wcm e f) (false,vs) -> f (true,v:vs)

%e f

%if e (false,vs) -> v and
%   f (false,vs) -> v' then

%e f (flag,vs) -> v v' (flag,vs)

%(wcm 1 (\x.x ccm))
%(wcm 1 (\x.x [1]))
%(wcm 1 [1])
%[1]

%(wcm 1 (\x.x (wcm 2 (ccm))))
%(wcm 1 (\x.x (wcm 2 [1,2])))
%(wcm 1 (\x.x [1,2]))
%(wcm 1 [1,2])
%[1,2]




\begin{proof}[Case $e=\mathrm{ccm}$]
receives a continuation and marks argument by 1
passes continuation and dummy variable by definition
by structural induction (assumption that it contains the marks?), this holds
\end{proof}

%Why does ccm have the same output as C[ccm] \x.\k.\m.(k x) \p.((p \x.\y.y) \k.\m.(k \x.\k.\m.(k \y.\k.\m.(k y))))
%proof of equivalence
%we need to state it formally and meaningfully

%since cm is a superset of lcv, any purely lcv term should reduce in cm to the same value 
%as if it were reduced in lcv and transformed

%we init the transformed program to initiate reduction

%given a program fragment and a continuation and marks state, produce the same as the 
%transformed program

%C[x] \y.y (false,[]) ->cm \k.\m.(k x)
%C[->lcv x] = \k.\m.(k x)

%->cm[C[x] \x.x (false,[])] = \k.\m.(k x)
%C[->lcv[x]] = \k.\m.(k x)

%Theorem is

%->cm[I[C[e]]] = C[->lcv[e]]

%where C is given as the transformation

%I[e] = \k.(k ((e \x.x) (false,C[empty]))) \x.\k.\m.(k x)
 
%How do we prove this? By induction on either the grammar or the reduction contexts?

%e=wcm e e
% ccm
% e e
% \x.e
% x

%E = wcm v F
%    F

%F = hole
%    wcm E e
%    E e
%    v E
    
%E e and v E are handled by the transform for application.


%for all continuations k and all marks (flag,ms) and all contexts E
%the value of variable x is C[x] (abstraction stripped away, then added)
%the value of ccm is ms
%ccm returns ms (do case by case)

%for wcm v (wcm e1 e2) -> (e1 -> v1 => wcm v (wcm e1 e2) -> wcm v1 e2

%\x.e v -> e[x<-v] (reason purely about CPSness)
%wcm v1 (wcm v2 e1) -> wcm v2 e1 (by reasoning about transform of wcm)

%transform of wcm:
%evaluate mark
%(true,cons mark if flag rest marks else marks)

%wcm v1 v2 -> v2 (by reasoning about transform of wcm, tail position guaranteed by passing continuation)

%E[ccm] = chi(E)

%-introduce lambdacm once again with its syntactic forms and evaluation rules (semantics)
%-introduce redex implementation of lambdacm
% -discuss reduction contexts and values
% -discuss call-by-value lambda calculus

%-discuss transformation of lambdacm to lambdav at a high level
% -discuss choice of CPS (evaluation control)
%-discuss properties the transformation should have

%-discuss intuitive approach to transformation
%-reason informally about special considerations

%-present transformation
%-formalize properties and state as theorems
%-prove each theorem

\section{C}
\subsection{properties of C}
\subsection{preservation theorem}
-informally analyze time/space complexity (via reductions)
-introduce ``direct-style'' transform (using static/dynamic continuation?)
\section{Future work}
-use this idea in compiler?

\chapter{Conclusion}
\bibliographystyle{plainnat}
\bibliography{dissertation}

\end{document}

% vim: lbr
