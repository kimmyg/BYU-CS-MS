\documentclass[ms,electronic,twosidetoc,letterpaper,chaptercenter,parttop]{byumsphd}
% Author: Chris Monson
%
% This document is in the public domain
%
% Options for this class include the following (* indicates default):
%
%   phd (*) -- produce a dissertation
%   ms -- produce a thesis
%
%   electronic -- default official university option, overrides the following:
%                 - equalmargins
%
%   hardcopy -- overrides the following:
%                 - no equalmargins
%                 - twoside
%
%   letterpaper -- ignored, but helpful for the Makefile that I use
%
%   10pt -- 10 point font size
%   11pt -- 11 point font size
%   12pt (*) -- 12 point font size
%
%   lof -- produce a list of figures in the preamble (off)
%   lot -- produce a list of tables in the preamble (off)
%   lol -- produce a list of listings in the preamble (off)
%
%   layout -- show layout lines on the pages, helps with overfull boxes (off)
%   grid -- show a half-inch grid on every page, helps with printing (off)
%   separator -- print an extra instruction page between preamble and body (off)
%
%   twoside (*) -- two-sided output (margins alternate for odd and even pages,
%     blank pages inserted to ensure that chapters begin on the right side of a
%     bound copy, etc.)
%   oneside -- one-sided output (margins are the same on all pages)
%   equalmargins -- make all margins equal - ugly for binding, but compliant
%
%   twosidetoc - start two-sided margins at the TOC instead of the body.  This
%     is sometimes (oddly) required, but be aware that it will make the page
%     numbering seem screwy, e.g., the first four full sheets of paper will
%     have number i-iv (not shown, though), and the next sheets will each have
%     two numbers, one for each side.  I suspect that most people don't look at
%     the roman numerals anyway, but it is a weird requirement.
%
%   openright (*) -- force new chapters to start on an odd page
%   openany -- don't use this, it's ugly
%
%   prettyheadings -- make the section/chapter headings look nice
%   compliantheadings (*) -- make them look ugly, but compliant with standards
%
%   chaptercenter -- center the chapter headings horizontally
%   chapterleft (*) -- place chapter headings on the left
%
%   partmiddle -- Part headers are centered vertically, no other text on page
%   parttop (*) -- Part headers at top of page, other text expected
%
%   duplexprinter -- Ensures that the two-sided portion starts on the right
%     side when printing.  This is not for use in submission, since the best
%     thing to do there is to print everything out one-sided, then take it down
%     to the copy store to have them do the rest.  It does help to save trees
%     when you are printing out copies just to look at them and fiddle with
%     things.
%
%
% EXAMPLES:
%
% The rest is up to you.  To fiddle with margins, use the \settextwidth and
% \setbindingoffset macros, described below.  I suggest that you
% \settextwidth{6.0in} for better-looking output (otherwise you'll get 3/4-inch
% margins after binding, which is sort of weird).  This will depend on the
% opinions of the various dean/coordinator folks, though, so be sure to ask
% them before embarking on a major formatting task.

% The following command fixes my particular printer, which starts 0.03 inches
% too low, shifting the whole page down by that amount.  This shifts the
% document content up so that it comes out right when printed.
%
% Discovering this sort of behavior is best done by specifying the ``grid''
% option in the class parameters above.  It prints a 1/2 inch grid on every
% page.  You can then use a ruler to determine exactly what the printer is
% doing.
%
% Uncomment to shift content up (accounting for printer problems)
%\setlength{\voffset}{-.03in}

% Here we set things up for invisible hyperlinks in the document.  This makes
% the electronic version clickable without changing the way that the document
% prints.  It's useful, but optional.
%
% NOTE: "driverfallback=ps2pdf" chooses ps2pdf in the case of LaTeX and pdftex
% in the case of pdflatex. If you use my LaTeX makefile (at
% http://latex-makefile.googlecode.com/) then pdftex is the default There are
% many other benefits to using the makefile, too.  This option is not always
% available, so use with care.
\usepackage[
    bookmarks=true,
    bookmarksnumbered=true,
    breaklinks=false,
    raiselinks=true,
    pdfborder={0 0 0},
    colorlinks=false,
    plainpages=false,
    ]{hyperref}

% To fiddle with the margin settings use the below.  DO NOT change stuff
% directly (like setting \textwidth) - it will break subtle things and you'll
% be tearing your hair out.
%
% For example, if you want 1.5in equal margins, or 2in and 1in margins when
% printing, add the following below:
%
%\setbindingoffset{1.0in}
%\settextwidth{5.5in}
%
% When equalmargins is specified in the class options, the margins will be
% equal at 1.5in each: (8.5 - 5.5) / 2.  When equalmargins is not specified,
% the inner margin will be 2.0 and the outer margin will be 1.0: inner = (8.5 -
% 5.5 - 1.0) / 2 + 1.0 (the 1.0 is the binding offset).
%
% The idea is this: you determine how much space the text is going to take up,
% whether for an electronic document (equalmargins) or not.  You don't want the
% layout shifting around between printed and electronic documents.
%
% So, you specify the text width.  Then, if there is a binding offset (when
% binding your thesis, the binding takes up space - usually 0.5 inches), that
% reduces the visual space on the final printed copy.  So, the *effective*
% margins are calculated by reducing the page size by the binding offset, then
% computing the remaining space and dividing by two.  Adding back in the
% binding offset gives the inner margin.  The outer margin is just what's left.
%
% All of this is done using the geometry package, which should be manipulated
% directly at your peril.  It's best just to use the above macros to manipulate
% your margins.
%
% That said, using the geometry macro to set top and bottom margins, or
% anything else vertical, is perfectly safe and encouraged, e.g.,
%
%\geometry{top=2.0in,bottom=2.0in}
%
% Just don't fiddle with horizontal margins this way.  You have been warned.

% This makes hyperlinks point to the tops of figures, not their captions
\usepackage[all]{hypcap}

% These packages allow the bibliography to be sorted alphabetically and allow references to more than one paper to be sorted and compressed (i.e. instead of [5,2,4,6] you get [2,4-6])
\usepackage[numbers,sort&compress]{natbib}
\usepackage{hypernat}

% Because I use these things in more than one place, I created new commands for
% them.  I did not use \providecommand because I absolutely want LaTeX to error
% out if these already exist.
\newcommand{\Title}{A CPS-like Transformation of Continuation Marks}
\newcommand{\Author}{Kimball R. Germane}
\newcommand{\GraduationMonth}{September}
\newcommand{\GraduationYear}{2012}

% Set up the internal PDF information so that it becomes part of the document
% metadata.  The pdfinfo command will display this.
\hypersetup{%
    pdftitle=\Title,%
    pdfauthor=\Author,%
    pdfsubject={MS Dissertation, BYU CS Department: %
                Degree Granted \GraduationMonth~\GraduationYear, Document Created \today},%
    pdfkeywords={BYU, thesis, dissertation, LaTeX},%
}

% Rewrite the itemize, description, and enumerate environments to have more
% reasonable spacing:
\newcommand{\ItemSep}{\itemsep 0pt}
\let\oldenum=\enumerate
\renewcommand{\enumerate}{\oldenum \ItemSep}
\let\olditem=\itemize
\renewcommand{\itemize}{\olditem \ItemSep}
\let\olddesc=\description
\renewcommand{\description}{\olddesc \ItemSep}

% Important settings for the byumsphd class.
\title{\Title}
\author{\Author}

\committeechair{Jay McCarthy}
\committeemembera{Sean Warnick}
\committeememberb{a}
\committeememberc{b}
\committeememberd{c}

\monthgraduated{\GraduationMonth}
\yeargraduated{\GraduationYear}
\yearcopyrighted{\GraduationYear}

\documentabstract{%
This document is an example of how to use the byumsphd {\LaTeX} class file.  The class creates Ph.D. and Master's documents equally well, producing all appropriate preamble content and adhering precisely to the minimum formatting requirements.  It is meant to replace the old ECEn style file that has been circulating for many years.

Note that there is a blank line between paragraphs, here.
}

\documentkeywords{%
    Thesis template, poorly-crafted example, conflicting margin instructions
}

\acknowledgments{%
    Thanks go to the ECEn style file authors for providing both a reasonable initial style and the motivation to abandon it.
}

\department{Computer~Science}
\graduatecoordinator{Dan~Ventura}
\collegedean{Thomas~W.~Sederberg}
\collegedeantitle{Associate~Dean}

% Customize the name of the Table of Contents section.
\renewcommand\contentsname{Table of Contents}

% Remove all widows an orphans.  This is not normally recommended, but in a
% paper dissertation there is no reasonable way around it; you can't exactly
% rewrite already-published content to fix the problem.
\clubpenalty 10000
\widowpenalty 10000

% Allow pages to have extra blank space at the bottom in order to accommodate
% removal of widows and orphans.
\raggedbottom

% Produce nicely formatted paragraphs. There is nothing additional to do.  In
% case you get some problems, surround your text with
% \begin{sloppy} ... \end{sloppy}. If that does not work, try
% \microtypesetup{protrusion=false} ... \microtypesetup{protrusion=true}
\usepackage{microtype}

\usepackage{amsmath}
%\usepackage{slatex}

\begin{document}

% Produce the preamble
\microtypesetup{protrusion=false}
\maketitle
\microtypesetup{protrusion=true}

\section{Introduction}

Thesis: A CPS-like global transformation can compile the $\lambda$-calculus with
continuation marks into the plain $\lambda$-calculus in a semantics-preserving way.

Continuation marks \cite{clements2006portable} are a language feature that provides a
mechanism to annotate the dynamic context of a program. This feature allows the
association of arbitrary values with arbitrary keys for the lifetime of an execution
context and the inquiry of currently defined values of pending contexts for a given set of
keys. This is advantageous for programs that require dynamic information about a program
execution such as debuggers, profilers, and steppers because it allows them to be defined
at the same level as the language instead of some level below.

The continuation-passing style (CPS) transformation is actually a family of 
transformations designed to make certain analyses simpler. The simplest variation of the 
CPS transformation augments each function with an additional formal parameter, the 
\emph{continuation}, which is a functional representation of currently pending 
computation. Functions in CPS never return; instead, they call the continuation argument 
with their result. The CPS transformation then simplifies programs by representing all 
control and data transfer uniformly and explicitly. In general, the ``spirit'' of the 
CPS transformation is to represent all transfers of control uniformly \cite{sabry1994formal}.

It is our interest to understand the essence of continuation marks--their behavior in the 
absense of other language features and implementation details. For this, we take the core 
of computation, the $\lambda$-calculus, and add facilities to manipulate continuation 
marks. These two together comprise a language which we term $\lambda_{cm}$. By expressing 
$\lambda_{cm}$ in terms of the plain $\lambda$-calculus, we uncover the meaning of 
continuation marks in a pure computational language. We arrive at this expression by 
performing a transformation in the spirit of CPS from $\lambda_{cm}$ to the $\lambda$-calculus 
verified to preserve the meaning of the source language.

\section{Continuation marks}

There are certain tools that are indispensable to some programmers that concern the
behavior of their programs: debuggers, profilers, steppers, etc. Without these tools,
these programmers cannot justify the adoption of a language, however compelling it might
otherwise be. Traditionally, these tools are developed at the same level as the 
language, privy to incidental implementation detail, precisely because that detail 
enables these tools to function. This is problematic for at least two reasons. First, 
it couples the implementation of the tool with the implementation of the language, which
increases the cost to port to other platforms. If users become dependent upon these tools,
it can stall the advancement of the language and the adoption of new language features.
Second and more critical, it makes these tools unsound. For instance, debuggers typically
examine programs which have been compiled without optimizations. In general, this means 
that the debugged program has different behavior than the deployed program. This is 
obviously undesirable.

It is desirable to implement such tools at the same level as the language, removing
dependency upon the implementation an instead relying on definitional and behavioral
invariants. Continuation marks are a language-level feature that provide the information
necessary for these tools to function. Furthermore, languages which require stack
inspection to enforce security policies (\emph{Java}, \emph{C\#}) or support aspect
oriented programming (\emph{aspectj}) can be defined in terms of a simpler language with
continuation marks.

Continuation marks originated in PLT Scheme (now Racket \cite{plt-tr1}) as a stack 
inspection mechanism. In fact, the \emph{Java} and \emph{C\#} languages rely on a similar 
stack inspection to enforce security policies of which continuation marks can be seen as 
a generalization. Surprisingly, continuation marks can be encoded in any language with 
exception facilities \cite{pettyjohn2005continuations} which fact has led to their 
experimental addition to Javascript \cite{clements2008implementing}.

The feature of continuation marks itself is accessible via two surface level syntactic
forms: \emph{with-continuation-mark} and \emph{current-continuation-marks}.

\emph{with-continuation-mark} has three parameters: a key expression \emph{key-expr}, a 
value expression \emph{value-expr} and a body expression \emph{body-expr}. The evaluation 
of \emph{value-expr} will be associated with a key, the evaluation of \emph{key-expr}, 
before the evaluation of \emph{body-expr}. During the lifetime of the evaluation of 
\emph{body-expr}, a continuation mark will exist associated with this key. In a Scheme-like 
syntax, this call appears like so:

% will use slatex for the dissertation; too much of a hassle now
\texttt{(with-continuation-mark} \emph{key-expr} \emph{value-expr} \emph{body-expr}\texttt{)}

\emph{current-continuation-marks} has one parameter, a set of keys \emph{key-set}, and returns a list of
all the associated values attached to the dynamic context of the invocation. If a particular 
context has been annotated by more than one key in the set, this will be reflected in the 
returned list\footnote{The returned list is typically a list of non-empty lists where each 
sub-list represents the marks on a context.}. Additionally, the order in which values were 
attached with a particular key is preserved. Scheme-like, \emph{current-continuation-marks}
looks like this:

\texttt{(current-continuation-marks} \emph{key-set}\texttt{)}

Importantly, the result of \emph{current-continuation-marks} provides no evidence of any portion of the
dynamic context lacking continuation marks with the specified keys. This preserves the
ability to perform optimizations without exposing details which would render the
optimizations unsound. This also requires special consideration of a language that
supports tail call optimization (which is not an optimization in the above sense since its
behavior is defined in the semantics of the language). By definition, \emph{body-expr} is 
in tail position; a language with tail call optimization will reflect this.

The canonical example to illustrate the behavior of continuation marks in the presence and
absence of proper tail recursion is the factorial function.

Figure \ref{fac-rec} illustrates the definitional recursive variant of the factorial
function. In this actualization, a cascade of multiplication operations builds as the
recursive calls are made. Each multiplication is computation that must be performed after
the recursive call of which the machine must keep track.

\begin{figure}
\begin{verbatim}
(define (fact n)
  (if (= n 0)
      1
      (* n (fact (- n 1)))))
\end{verbatim}
\caption{The definitionally recursive factorial function}
\label{fac-rec}
\end{figure}

Figure \ref{fac-tail-rec} illustrates the tail recursive manifestation of the factorial
function. In contrast to the function in figure \ref{fac-rec}, this formulation performs
the multiplication before the recursive call. Because the function has no pending
computations after the evaluation of the recursive call, the execution context need not
grow. Such a call is said to be in tail position.

\begin{figure}
\begin{verbatim}
(define (fact-tr n acc)
  (if (= n 0)
      acc
      (fact-tr (- n 1) (* n acc))))
\end{verbatim}
\caption{A tail-recursive variant of the factorial function}
\label{fac-tail-rec}
\end{figure}

Figures \ref{fac-rec-cm} and \ref{fac-tail-rec-cm} represent these two variants of the
factorial function augmented with continuation marks. Using these definitions, the 
result of \texttt{(fact 3)} would be

\begin{verbatim}
(((fact 1)) ((fact 2)) ((fact 3)))
6
\end{verbatim}

whereas the result of \texttt{(fact-tr 3 1)} would be

\begin{verbatim}
(((fact 1)))
6
\end{verbatim}.

\begin{figure}
\begin{verbatim}
(define (fact n)
  (if (= n 0)
      (begin
        (display (current-continuation-marks '(fact)))
        1)
      (with-continuation-mark 'fact n (* n (fact (- n 1)))))
\end{verbatim}
\caption{The definitionally recursive factorial function augmented with continuation marks}
\label{fac-rec-cm}
\end{figure}

\begin{figure}
\begin{verbatim}
(define (fact-tr n acc)
  (if (= n 0)
      (begin
        (display (current-continuation-marks '(fact)))
        acc)
      (with-continuation-mark 'fact n (fact-tr (- n 1) (* n acc))))
\end{verbatim}
\caption{The tail-recursive factorial function augmented with continuation marks}
\label{fac-tail-rec-cm}
\end{figure}

This difference is due to the growing continuation in the definitionally recursive
\texttt{fact}. Each call to \texttt{fact} has a pending computation--namely, the
multiplication--after the recursive call and so each necessitates the creation an
additional evaluation context. The effect of these additional contexts is that each
annotation is applied to a new, ``blank'' context, so all the annotations are preserved. 
In the tail-recursive variant, there is no pending computation and therefore no additional
evaluation context. In this instance, the previous mark is overwritten with the new.

\section{CPS transformations}

The CPS transformation is a family of language transformations derived from Plotkin
\cite{plotkin1975call} and designed to simplify  programs by representing all data and
control flow uniformly and explicitly, in turn simplifying compiler construction and
analyses such as optimization and verification \cite{sabry1994formal}. The standard
variation adds a formal parameter to every function definition and an argument to every
call site.

As an example, consider once again the two variants of the factorial function, sans
continuation marks, given earlier. In CPS, the properly recursive variant can be expressed
as
\begin{verbatim}
(define (fact n k) 
  (if (= n 0)
      (k 1)
      (fact (- n 1) (lambda (acc) (k (* n acc))))))
\end{verbatim}
and the tail-recursive variant as
\begin{verbatim}
(define (fact-tr n acc k)
  (if (= n 0)
      (k acc)
      (fact-tr (- n 1) (* n acc) k)))
\end{verbatim}. 
(For clarity, we have treated ``primitive'' functions--equality comparison, subtraction,
and multiplication--in a direct manner. In contrast, a full CPS transformation would
affect \emph{every} function.)

Notice that, in the first variation, each recursive call receives a newly-constructed $k$
encapsulating additional work to be performed at the completion of the recursive
computation. In the second, $k$ is passed unmodified, so while computation occurs within
each context, no \emph{additional} computation pends. From this example, we see that the
CPS representation is ideal for understanding tail-call behavior as it is explicit that
the continuation is preserved by the tail call.

The purpose of CPS does not lie solely in pedagogy, however. The reification of and
consequent ability to directly manipulate the continuation is a powerful ability,
analogous in power to the ability to \emph{capture} a continuation which some languages
provide. In Scheme, this is accomplished with \emph{call/cc}, short for ``call with
current continuation''. This call takes one argument which itself is a function of one
argument. \emph{call/cc} calls its argument, passing in a functional representation of the
current continuation--the continuation present when \emph{call/cc} was invoked. This
continuation function takes one argument which is treated as the result of \emph{call/cc}
and runs this continuation to completion.

As a simple example,
\begin{verbatim}
(+ 1 (call/cc
       (lambda (k)
         (k 1))))
\end{verbatim}
returns $2$. In effect, invoking $k$ with the value 1 is the same as replacing the entire
\emph{call/cc} invocation with the value 1.

Much of the power of \emph{call/cc} lies in the manifestation of the continuation as a
function, giving it first-class status. It can be passed as an argument in function calls,
invoked, and, amazingly, reinvoked at leisure. It is this reinvokeability that makes
\emph{call/cc} the fundamental unit of control from which all other control structures can
be built, including generators, coroutines, and threads.

%\emph{call/cc} is erroneously seen as incredibly heavyweight and overkill for %control
(cite something). This conception probably comes from the conceptualization %of the
continuation as the call stack and continuation capture as stack copy while %continuation
call is stack installation. It is also seen as a form of \emph{goto} %which is known to
obfuscate control flow and impede analysis (cite something). %[Can't and shouldn't talk
much about the second. Probably take it out.] Bearing in %mind that the CPS transformation
aids compilers, it is useful to investigate the %characterization of \emph{call/cc} within
the standard CPS transformation.

In direct style, the definition of \emph{call/cc} is conceptually 
\begin{verbatim}
(define call/cc
  (lambda (f)
    (f (get-function-representing-continuation))))
\end{verbatim}
where \emph{get-function-representing-continuation} is an opaque function which leverages
sweeping knowledge of the language implementation. The CPS definition is notably easier:
\begin{verbatim}
(define call/cc
  (lambda (f k)
    (f k k)))
\end{verbatim}

Variations on the standard CPS transformation can make the expression of certain control
structures more straightforward. For instance, the ``double-barrelled'' CPS transformation
is a variation wherein each function signature receives not one but two additional formal
parameters, each a continuation. One application of this particular variation is error
handling with one continuation argument representing the remainder of a successful
computation and the other representing the failure contingency. It is especially useful in
modelling exceptions and other non-local transfers of control in situations where the
computation might fail. In general, the nature of the CPS transformation allows it to
untangle complicated, intricate control structures.

Similar transformations exist which express other programming language features such as
security annotations \cite{wallach2000safkasi} and control structures such as procedures,
exceptions, labelled jumps, coroutines, and backtracking. On top of other offerings, this
places it in a category of tools to describe and analyze programming language features.
(This category is also occupied by Moggi's computational lambda calculus--monads
\cite{moggi1989computational}.)

One of the reasons CPS is useful is that is allows evaluation order to be controlled. This
is done by a definitional invariant: every term applied to a continuation returns a value.
We will visit each case of the definition and verify this property. Before doing so, we
emphasize that no reduction is performed by effect of the transformation.

\[
T[x]=\lambda k.(k x)
\]

Variables are values, so the property holds immediately here.

\[
T[\lambda x.E]=\lambda k.(k \lambda x.T[E])\]

Here we reason inductively by assuming that the property holds for E. Abstractions are values, so the 
property holds in this case also.

\[
T[E F]=\lambda k.(T[E] (\lambda e.(T[F] (\lambda f.((e f) k))))
\]

Once again, we assume that the property holds for E and F. Then e and f are values. In order to conclude 
that the property holds for the application, we must establish that the application of values yields a 
value. The key lies in a case analysis of the operator. It must be either a variable or an abstraction.

First, consider the application of a variable to a variable.

\[
x y
\]

This is fully reduced according to direct application.

\begin{align*}
T[x y] &= \lambda k.(T[x] (\lambda e.T[y] (\lambda f.((e f) k))))\\
       &= \lambda k.(\lambda k.(k x) (\lambda e.T[y] (\lambda f.((e f) k))))\\
       &= \lambda k.(\lambda k.(k x) (\lambda e.\lambda k.(k y) (\lambda f.((e f) k))))\\
\end{align*}

We can evaluate this expression by applying it to the identity function $\lambda z.z$.

\begin{align*}
&\rightarrow \lambda k.(\lambda k.(k x) (\lambda e.\lambda k.(k y) (\lambda f.((e f) k)))) \lambda z.z\\
&\rightarrow \lambda k.(k x) (\lambda e.\lambda k.(k y) (\lambda f.((e f) \lambda z.z)))\\
&\rightarrow \lambda e.\lambda k.(k y) (\lambda f.((e f) \lambda z.z)) x\\
&\rightarrow \lambda k.(k y) (\lambda f.((x f) \lambda z.z))\\
&\rightarrow \lambda f.((x f) \lambda z.z) y\\
&\rightarrow (x y) \lambda z.z\\
\end{align*}

Similar to a direct-style application, we remain stuck.

Now, consider the application of an abstraction to a variable.

\[
\lambda x.x y
\]

This reduces to y, of course, when directly applied.

\begin{align*}
T[\lambda x.x y] &= \lambda k.(T[\lambda x.x] (\lambda e.(T[y] (\lambda f.((e f) k))))\\
                 &= \lambda k.(\lambda k.(k \lambda x.T[x]) (\lambda e.(T[y] (\lambda f.((e f) k))))\\
                 &= \lambda k.(\lambda k.(k \lambda x.\lambda k.(k x)) (\lambda e.(T[y] (\lambda f.((e f) k))))\\
                 &= \lambda k.(\lambda k.(k \lambda x.\lambda k.(k x)) (\lambda e.\lambda k.(k y) (\lambda f.((e f) k))))\\
\end{align*}

Apply the identity function $\lambda z.z$ to this term.

\begin{align*}
&\rightarrow \lambda k.(\lambda k.(k \lambda x.\lambda k.(k x)) (\lambda e.\lambda k.(k y) (\lambda f.((e f) k)))) \lambda z.z\\
&\rightarrow \lambda k.(k \lambda x.\lambda k.(k x)) (\lambda e.\lambda k.(k y) (\lambda f.((e f) \lambda z.z)))\\
&\rightarrow (\lambda e.\lambda k.(k y) (\lambda f.((e f) \lambda z.z))) \lambda x.\lambda k.(k x)\\
&\rightarrow \lambda k.(k y) (\lambda f.((\lambda x.\lambda k.(k x) f) \lambda z.z))\\
&\rightarrow \lambda f.((\lambda x.\lambda k.(k x) f) \lambda z.z) y\\
&\rightarrow (\lambda x.\lambda k.(k x) y) \lambda z.z\\
&\rightarrow \lambda k.(k y) \lambda z.z\\
&\rightarrow \lambda z.z y\\
&\rightarrow y\\
\end{align*}

We observe that, at the time of application, the remnant of the transformation of $\lambda x.x$ took the form 
$\lambda x.\lambda k.(k x)$. This is a term which, when applied, will yield a value. We can generalize our reasoning 
to establish this property for all applications by considering

$T[\lambda x.E F]$

and noting that E also satisfies this property.

[could perhaps do something more detailed and formal here]

Suppose we want to embed direct-style terms in CPS terms. We are interested in what cases we can induce 
reduction.

As an example which will become useful later, suppose we want to deal with a Church encoding of lists 
directly.

Recall that we can represent \emph{nil} as [the Church encoding of] \emph{false} and \emph{cons a b} as 
$\lambda p.((p a) b)$.

Like [the transformation] T, C has a similar property which we quickly verify:

\begin{align*}
C[x]           &= \lambda k.\lambda m.(k x)\\
C[\lambda x.E] &= \lambda k.\lambda m.(k \lambda x.C[E])\\
C[E F]         &= \lambda k.\lambda m.((C[E] (\lambda e.((C[F] (\lambda f.(((e f) k) m))) g[m]))) g[m])\\
C[wcm E F]     &= \lambda k.\lambda m.((C[E] (\lambda e.((C[F] k) h[e,m]))) g[m])\\
C[ccm]         &= \lambda k.\lambda m.(k i[m])\\
\end{align*}

where g[m]=cons false (snd m)
and h[e,m]=cons true (if fst m then rest (snd m) else snd m)
and i[m]=snd m

We use the definitions $g$, $h$, and $i$ to emphasize that these facilities enjoy privileged status 
in the transformation definition and are invisible to the program.

-uses
-lambda calculus
-reference CPS due to Fischer/Plotkin
-properties

\chapter{$\lambda_{cm}$}

We consider an extension of the call-by-value $\lambda$-calculus with facilities to
manipulate continuation marks, introduced by Pettyjohn et al.
\cite{pettyjohn2005continuations}, which we term $\lambda_{cm}$.

\section{Definition of $\lambda_{cm}$}

Figure \ref{language-syntax} presents the syntactic definition of $\lambda_{cm}$.
Definitions of $E$ and $F$ signify evaluation contexts. The non-terminal $E$ is simply any
term in the language but for the constraint that evaluation contexts interleave \emph{wcm}
directives. The definition of $e$ establishes the forms of valid expressions in the
language: applications, variables, values, \emph{wcm} forms, and \emph{ccm} forms. The
definition of $v$ denotes that values in the language are $\lambda$-abstractions. (Also,
notice that \emph{wcm} and \emph{ccm} have one parameter fewer than the forms introduced
earlier. This is because $\lambda_{cm}$ expresses unkeyed marks. While it can be shown
that a language with keyed marks can be expressed in terms of a language with unkeyed
marks, such as $\lambda_{cm}$, this is not our concern.)

\begin{figure}
\begin{align*}
E = &(\mathrm{wcm}\,v\,F) & e = &(e\,e)\\
    &F                    &     &x\\
F = &[]                   &     &v\\
    &(E\,e)               &     &(\mathrm{wcm}\,e\,e)\\
    &(v\,E)               &     &(\mathrm{ccm})\\
    &(\mathrm{wcm}\,E\,e) & v = & \lambda x. e
\end{align*}
\caption{$\lambda_{cm}$ syntax}
\label{language-syntax}
\end{figure}

Figure \ref{language-semantics} presents the semantics of $\lambda_{cm}$. The definitions
therein establish the proper interpretation of various expressions. The first follows the
typical definition of application. The second defines the tail behavior of the \emph{wcm}
form. The third expresses that the \emph{wcm} form takes on the value of its body.
Finally, the fourth defines the value of the \emph{ccm} form in terms of the $\chi$
metafunction which definition is given in figure \ref{chi-metafunction}. The $\chi$
metafunction is defined over syntactic forms of the language, so its definition
corresponds closely to the definition of $\lambda_{cm}$ found in figure
\ref{language-syntax}.

\begin{figure}
\begin{align*}
E[(\lambda x.e)\,v]                         &\rightarrow E[e[x\leftarrow v]]\\
E[(\mathrm{wcm}\,v\,(\mathrm{wcm}\,v'\,e))] &\rightarrow E[(\mathrm{wcm}\,v'\,e)]\\
E[(\mathrm{wcm}\,v\,v')]                    &\rightarrow E[v']\\
E[(\mathrm{ccm})]                           &\rightarrow E[\chi(E)]
\end{align*}
\caption{$\lambda_{cm}$ evaluation rules}
\label{language-semantics}
\end{figure}

\begin{figure}
\begin{align*}
\chi([])                   &= \mathrm{empty}\\
\chi((E\,e))               &= \chi(E)\\
\chi((v\,E))               &= \chi(E)\\
\chi((\mathrm{wcm}\,E\,e)) &= \chi(E)\\
\chi((\mathrm{wcm}\,v\,E)) &= v : \chi(E)
\end{align*}
\caption{Definition of $\chi$ metafunction}
\label{chi-metafunction}
\end{figure}

\section{A Redex interpreter for $\lambda_{cm}$}

Redex \cite{findler2010redex} is a domain-specific language for exploring language
semantics. It lives very close to the semantics notation we have used in this discussion.
To illustrate how easily langagues can be defined in Redex, we will examine a Redex
program which defines a toy language. In contrast to a Redex tutorial, we will not concern
ourselves with the syntax and structure of roads not taken and will instead briefly
explain each component of the program.

\begin{verbatim}
(define-language toy
  (x variable-not-otherwise-mentioned)
  (v number undefined) 
  (e (+ e e) (with (x e) e) x v)
  (E hole (+ E e) (+ v E) (with (x E) e)))
\end{verbatim}

This expression defines the abstract syntactic structure of a language named \emph{toy}.
There are four categories of structures: $x$, $v$, $e$, and $E$. The category $x$ is
defined to contain any token not otherwise mentioned in the definition. The category $v$
is defined to contain numbers and the token \emph{undefined}. The category $e$ is defined
to contain the expression forms of the language, of which there are four: addition
expressions, \emph{with} expressions, lone variables, and lone values. The last category,
$E$, does not define abstract syntax but instead reduction contexts. The first reduction
context is a \emph{hole} (a special token in Redex) which will be filled in with the
result of the expression that previously resided in its place. The next two are addition
contexts, the first representing the evaluation of the first argument and the second
representing the evaluation of the second; the composition of these contexts imposes an
order on the evaluation of the arguments. The final context, a \emph{with} context,
specifies a variable, a value, and an expression within which that variable is bound to
that value.

\begin{verbatim}
(define toy-rr
  (reduction-relation
   toy
   (--> (in-hole E (+ number_1 number_2))
        (in-hole E ,(+ (term number_1) (term number_2)))
        "+")
   (--> (in-hole E (with (x_1 v_1) e_1))
        (in-hole E (substitute x_1 v_1 e_1))
        "with")
   (--> (in-hole E x_1)
        (in-hole E undefined)
        "free variable")
   (--> (in-hole E (+ undefined e_1))
        (in-hole E undefined)
        "undefined in first position")
   (--> (in-hole E (+ number_1 undefined))
        (in-hole E undefined)
        "undefined in second position")))
\end{verbatim}

This term defines a reduction relation on the \emph{toy} language. The five defined
reductions, signalled by \texttt{-->}, match specified patterns and manipulate them
according to the defined rules. These define: the addition of two numbers; the
substitution of a \emph{with} expression; a lone variable; the addition of an undefined
value on the left; and the addition of an undefined value on the right.

\begin{verbatim}
(define-metafunction toy
  substitute : x v e -> e
  [(substitute x_1 v_1 (+ e_1 e_2))
   (+ (substitute x_1 v_1 e_1) (substitute x_1 v_1 e_2))]
  [(substitute x_1 v_1 (with (x_1 e_1) e_2))
   (with (x_1 (substitute x_1 v_1 e_1)) e_2)]
  [(substitute x_1 v_1 (with (x_2 e_1) e_2))
   (with (x_2 (substitute x_1 v_1 e_1)) (substitute x_1 v_1 e_2))]
  [(substitute x_1 v_1 x_1)
   v_1]
  [(substitute x_1 v_1 x_2)
   x_2]
  [(substitute x_1 v_1 v_2)
   v_2])
\end{verbatim}

The definition of the \emph{with} reduction rule relies on the \emph{substitute}
metafunction. (Metafunctions assist in defining the language, and therefore exist outside
of that language; hence, they are not functions of the language but metafunctions.) The
\emph{substitute} metafunction recursively substitutes a variable in an expression with a 
value. The substitution is only propagated as long as a binding with the same name is not 
encountered. At that point, the substitution is performed in the value expression of the 
binding, but not the body. This allows for expressions like
\begin{verbatim}
(with (x 5)
  (with (x x)
    x))
\end{verbatim}
to behave as we expect.

Now that the syntactic forms and reduction rules of the language are defined, we can use 
the randomized testing built into Redex to investigate properties of the language. We 
start by defining the helper function \emph{reduces-to-one-value?} (which has its own 
helper function \emph{value?}).
\begin{verbatim}
(define value? (redex-match toy v))

(define (reduces-to-one-value? e)
  (let ((results (apply-reduction-relation* toy-rr e)))
    (and (= (length results) 1)
         (value? (first results)))))
\end{verbatim}
The \emph{*} at the end of the function name \emph{apply-reduction-relation*} signifies
that all possible reduction rules will be applied as many times as possible. If some of
the reduction rules don't actually reduce, the relation may produce a reducible term
indefinitely. The function \emph{apply-reduction-relation*} is in a sense strict in the
reduction relation and will likewise run indefinitely if this is the case.

After the language and some properties have been established, the randomized testing, 
initiated by
\begin{verbatim}
(redex-check toy
             e
             (reduces-to-one-value? (term e)))
\end{verbatim}
is simple. We merely provide the name of the language we wish to work with, the
nonterminal in the grammar we wish to use to generate language terms, and a predicate that
checks terms for properties. This function generates terms gradually increasing in size,
applying the predicate to each in turn, and terminates with a counterexample or after a
set number of terms have been checked (1000 by default).











We will present a transformation in the spirit of CPS from $\lambda_{cm}$ to 
the plain call-by-value $\lambda$-calculus. Furthermore, we will provide and 
prove a meaning-preservation theorem.

\subsection{Example CPS transform}

As an example of a CPS transform definition, we supply Fischer's transform (which we call
$\mathcal{F}$) of the $\lambda$-calculus \cite{fischer1972lambda} below. Recall that terms
in the $\lambda$-calculus take the form of lone variables $x$, $\lambda$-abstractions
$\lambda x.M$, and applications $M\,N$ where $M$ and $N$ are themselves $\lambda$-calculus
terms.

\begin{align*}
\mathcal{F}[x]           &= \lambda k.k\,x\\
\mathcal{F}[\lambda x.M] &= \lambda k.k(\lambda x.\mathcal{F}[M])\\
\mathcal{F}[M\,N]       &= \lambda k.\mathcal{F}[M](\lambda m.\mathcal{F}[N](\lambda n.(m\,n)\,k))
\end{align*}


Fischer's CPS transformation abstracts each term in the $\lambda$-calculus: lone variables
wait on a continuation, abstractions receive a degree of indirection, and even
applications, the sole reduction facility of the $\lambda$-calculus, become abstractions.
In essence, terms become suspended in wait of a continuation argument. By priming a term
so-transformed with a continuation function, even as simple as the identity, we instigate
a cascade of computation. In the case of lone variables and applications, we see that this
transformation has no effect on the result. However, the values of the
$\lambda$-calculus--abstractions--are contaminated by the transform. For example

\[
T[\lambda x.x]=\lambda k.(k \lambda x.\lambda k.(k x))
\].

It is for this reason that we must take special care with our transform which is based on Plotkin's.

Our use of call-by-value $\lambda$-calculus has assumed a distinction between terms which
represent values and terms which do not. A term is a \emph{value} if it cannot be reduced
further. This alone is not a satisfactory definition because it depends on when reduction
can occur. In our definition, we consider both $\lambda x.x$ and $\lambda
x.(\lambda y.y\,x)$ to be fully reduced (we consider them to be values, we cannot reduce
either of these terms). [something about weak head normal form here] The Fischer-Plotkin 
CPS transform allows us to transform the latter term into a term which can be evaluated as 
fully as we would expect while still respecting our notion of reduction contexts. It may 
be beneficial to show this:

\begin{align*}
\mathcal{F}[\lambda x.(\lambda y.y\,x)] &= \lambda k.(k\,\lambda x.\mathcal{F}[\lambda y.y\, x])\\
&= \lambda k.(k\, \lambda x.(()))
\end{align*}

The evaluation rules for $\lambda_{cm}$ require us to distinguish between expressions and values

For simple variables and abstractions, we use Plotkin's definitions almost exactly.

\begin{align*}
C[x]    &= \lambda k.\lambda m.(k\,x)\\
C[\lambda x.M] &= \lambda k.\lambda m.(k\,\lambda x.C[M])
\end{align*}

In the $\lambda$-calculus, evaluation occurs during reduction, and reduction is merely
application. There is, however, some subtlety in which contexts we allow applications to 
be reduced. For example, in the call-by-value $\lambda$-calculus, $\lambda y.y\,x$ is 
reducible; by contrast, $\lambda x.(\lambda y.y\,x)$ is not. We might imagine a system in 
which we, by convention, reduce beneath abstractions. In such a system, the latter term, 
when applied to $z$, would reduce as

\begin{align*}
            &\lambda x.(\lambda y.y\, x)\,z\\
\rightarrow &\lambda x.x\,z\\
\rightarrow &z
\end{align*}

whereas a call-by-value reduction would reduce as

\begin{align*}
            &\lambda x.(\lambda y.y\, x)\,z\\
\rightarrow &\lambda y.y\,z\\
\rightarrow &z
\end{align*}

Although both terms reduce to the same term in this example, this distinction is not 
merely pedantic: terms may reduce definitively in some reduction regimes and fail to 
reduce completely in others! [talk about reduction orders giving rise to call-by-value 
and call-by-name] [talk about CPS allowing us to simulate on in the other The term binding induced by this transform allowed Plotkin to simulate the call-by-value
dialect of the $\lambda$-calculus by the call-by-name and vice versa
\cite{plotkin1975call}.]

We assume a call-by-value reduction convention in $\lambda_{cm}$ and similarly in our 
target language, the $\lambda$-calculus, which we will denote $\lambda_{v}$.


\section{Language translation}

$\lambda_{cm}$ is a strict superset of $\lambda_{v}$; there are within $\lambda_{cm}$ 
syntactic forms inscrutable to a $\lambda_{v}$ interpreter. Additionally, evaluation 
order in $\lambda_{cm}$ is significant and differs from the call-by-value regime of the 
target language somewhat (which is to say, we cannot simply inherit it). Recall that one 
feature of the CPS transform, due to Plotkin \cite{plotkin1975call}, is to control 
evaluation order. We will use this facility in our transformation to express terms in 
$\lambda_{cm}$ in terms of $\lambda_{v}$. [Other analysis which the CPS transformation 
affords us \cite{appel2007compiling} is incidental.]

The work of the transformation then is to express the five syntactic forms of
$\lambda_{cm}$ ($e$-terms in figure \ref{language-syntax}) in the three syntactic forms of
$\lambda_{v}$ (the first three $e$-terms of that same figure) in a semantics-preserving
way. Much of the effort of our analysis and justification will be in demonstrating that
the semantics are indeed preserved, namely the tail-call behavior.

In continuation-passing style, every term takes an additional argument, the continuation,
into which the evaluation is passed. Our approach to a semantics-preserving
transformation, which we call $\mathcal{C}$, takes this a step further. Intuitively, we
will pass not only a continuation but also a list of the current continuation marks. The
\emph{wcm} form will add a mark to this list and the \emph{ccm} form will simply return it.

\subsection{First pass toward $\mathcal{C}$}

Let us denote a first pass toward $\mathcal{C}$ as $\mathcal{C}_{1}$. We might define 
$\mathcal{C}_{1}$ formally as 

\begin{align*}
\mathcal{C}_{1}[x]           &= \lambda k.\lambda m.(k\,x)\\
\mathcal{C}_{1}[\lambda x.E] &= \lambda k.\lambda m.(k\,\lambda x.\mathcal{C}_{1}[E])\\
\mathcal{C}_{1}[E\,F]        &= \lambda k.\lambda m.((\mathcal{C}_{1}[E]\,(\lambda e.((\mathcal{C}_{1}[F]\,(\lambda n.(((e\,f)\,k)\,m)))\,m)))\,m)\\
\mathcal{C}_{1}[wcm\,E\,F]   &= \lambda k.\lambda m.((\mathcal{C}_{1}[E]\,(\lambda e.((\mathcal{C}_{1}[F]\,k)\,\lambda p.((p\,e)\,m))))\,m)\\
\mathcal{C}_{1}[ccm]         &= \lambda k.\lambda m.(k\,m)
\end{align*}

Despite its gross deficiencies, this definition captures some of the critical aspects of a
correct transform, namely, the explicit ordering of evaluation in the \emph{wcm} form and
the conceptual simplicity of the \emph{ccm} form by virtue of our continuation
mark-passing approach.

say something about the order reversal of the marks

\subsection{What do you mean ``equivalent''?}

At this point, it may be profitable to discuss the properties of a correct transform from
$\lambda_{cm}$ to $\lambda_{v}$. Because $\lambda_{cm}$ is a superset of $\lambda_{v}$, we
should expect terms of $\lambda_{cm}$ that are also in $\lambda_{v}$ to be equivalent in
some sense. What sense? Consider the term
\[
\lambda x.x
\]
in $\lambda_{cm}$. As an abstraction, this term denotes a value and would expect, at very
least, it to be transformed into a value in $\lambda_{v}$. However, we need to keep in
mind that in continuation-passing style, \emph{all} terms are abstractions, and therefore
values. For, consider the term
\[
\lambda x.x\,\lambda y.y
\]
also in $\lambda_{cm}$. Fischer's CPS transformation of this is
\begin{align*}
            &\mathcal{F}[\lambda x.x\,\lambda y.y]\\
= &\lambda k.(\mathcal{F}[\lambda x.x]\,(\lambda e.\mathcal{F}[\lambda y.y]\,(\lambda f.((e\,f)\,k))))\\
= &\lambda k.(\lambda k.(k\,\lambda x.\mathcal{F}[x])\,(\lambda e.\mathcal{F}[\lambda y.y]\,(\lambda f.((e\,f)\,k))))\\
= &\lambda k.(\lambda k.(k\,\lambda x.\lambda k.(k\,x))\,(\lambda e.\mathcal{F}[\lambda y.y]\,(\lambda f.((e\,f)\,k))))\\
= &\lambda k.(\lambda k.(k\,\lambda x.\lambda k.(k\,x))\,(\lambda e.\lambda k.(k\,\lambda y.\mathcal{F}[y])\,(\lambda f.((e\,f)\,k))))\\
= &\lambda k.(\lambda k.(k\,\lambda x.\lambda k.(k\,x))\,(\lambda e.\lambda k.(k\,\lambda y.\lambda k.(k\,y))\,(\lambda f.((e\,f)\,k))))
\end{align*}
which, strictly speaking, produces a value. What we're really interested in is the form of 
the result of the computation and, despite the radical change that has taken place, no 
computation has been performed. Instead, the transformation produces a term that awaits 
a continuation--any continuation--to instigate evaluation. At the top level, we are simply 
interested in seeing the result so we apply this term to simply the identity function 
$\lambda z.z$. Reduction proceeds as
\begin{align*}
            &\lambda k.(\lambda k.(k\,\lambda x.\lambda k.(k\,x))\,(\lambda e.\lambda k.(k\,\lambda y.\lambda k.(k\,y))\,(\lambda f.((e\,f)\,k))))\,\lambda z.z\\
\rightarrow_{\lambda_{v}} &\lambda k.(k\,\lambda x.\lambda k.(k\,x))\,(\lambda e.\lambda k.(k\,\lambda y.\lambda k.(k\,y))\,(\lambda f.((e\,f)\,\lambda z.z)))\\
\rightarrow_{\lambda_{v}} &\lambda e.\lambda k.(k\,\lambda y.\lambda k.(k\,y))\,(\lambda f.((e\,f)\,\lambda z.z))\,\lambda x.\lambda k.(k\,x)\\
\rightarrow_{\lambda_{v}} &\lambda k.(k\,\lambda y.\lambda k.(k\,y))\,(\lambda f.((\lambda x.\lambda k.(k\,x)\,f)\,\lambda z.z))\\
\rightarrow_{\lambda_{v}} &\lambda f.((\lambda x.\lambda k.(k\,x)\,f)\,\lambda z.z)\,\lambda y.\lambda k.(k\,y)\\
\rightarrow_{\lambda_{v}} &(\lambda x.\lambda k.(k\,x)\,\lambda y.\lambda k.(k\,y))\,\lambda z.z\\
\rightarrow_{\lambda_{v}} &\lambda k.(k\,\lambda y.\lambda k.(k\,y))\,\lambda z.z\\
\rightarrow_{\lambda_{v}} &\lambda z.z\,\lambda y.\lambda k.(k\,y)\\
\rightarrow_{\lambda_{v}} &\lambda y.\lambda k.(k\,y)
\end{align*}
. We contrast this with the result of the direct-style evaluation, $\lambda y.y$, and notice 
that the CPS transformation, both global and thorough, has contaminated even the value our 
program produces. This observation will influence both the exact properties we should 
expect a correct transform should have and our construction of it.


The transformation $\mathcal{C}_{1}$ is fundamentally similar to a CPS transformation and 
so shares the attribute that transformed terms must be ``primed'' with a continuation in 
order for evaluation to proceed. In fact, terms produced by $\mathcal{C}_{1}$ are doubly 
abstracted, requiring an argument encoding the current continuation marks. For now, we
will assume that this approach is a fine way to go.

[say something here]

It is at this point that we notice that, however we choose to encode it, the empty list 
should initiate the program. Having established this, we can state some general facts about 
$\mathcal{C}$:

\begin{enumerate}
\item Like other CPS transformations, it contaminates values.
\item dasf
\end{enumerate}

Consider the program \emph{ccm}. In $\lambda_{cm}$, it evaluates to the empty list under 
some encoding; that is,
\[
\mathrm{ccm}\rightarrow_{\lambda_{cm}}empty
\]
. We also have
\[
\mathcal{C}_{1}[\mathrm{ccm}]=\lambda k.\lambda m.(k\,m)
\]
and assuming for now that the identity function is a satisfactory continuation to provide 
and letting $[]$ represent our encoding of the empty list, we have
\begin{align*}
            &(\lambda k.\lambda m.(k\,m)\,\lambda z.z)\,[]\\
\rightarrow_{\lambda_{v}} &\lambda m.(\lambda z.z\,m)\,[]\\
\rightarrow_{\lambda_{v}} &\lambda z.z\,[]\\
\rightarrow_{\lambda_{v}} &[]
\end{align*}
. Here we immediately observe that \emph{empty} and $[]$ should be equivalent in some
sense, and can concretely provide that sense:
\[
E\rightarrow_{\lambda_{cm}}v'\Rightarrow(\mathcal{C}[E]\,\lambda x.x)\,\mathcal{C}_{v}[\mathrm{empty}]\rightarrow_{\lambda_{v}}\mathcal{C}_{v}[v']
\]
That is, we expect that a transformed program should yield a value transformed by a
similar but not necessarily identical $\mathcal{C}$ which we call $\mathcal{C}_{v}$.


We now turn our attention to the tail-call behavior of $\mathcal{C}_{1}$. According to the 
semantics of $\lambda_{cm}$, the program 
$(\mathrm{wcm}\,a\,(\mathrm{wcm}\,b\,(\mathrm{ccm})))$ evaluates to b:
$(\mathrm{wcm} a (\lambda x.x (\mathrm{wcm}\,b\,(\mathrm{ccm}))))$. 

 
-introduce lambdacm once again with its syntactic forms and evaluation rules (semantics)
-introduce redex implementation of lambdacm
 -discuss reduction contexts and values
 -discuss call-by-value lambda calculus

-discuss transformation of lambdacm to lambdav at a high level
 -discuss choice of CPS (evaluation control)
-discuss properties the transformation should have

-discuss intuitive approach to transformation
-reason informally about special considerations

-present transformation
-formalize properties and state as theorems
-prove each theorem

\section{C}
\subsection{properties of C}
\subsection{preservation theorem}
-informally analyze time/space complexity (via reductions)
-introduce ``direct-style'' transform (using static/dynamic continuation?)
\section{Future work}
-use this idea in compiler?
\section{Conclusion}
\bibliographystyle{plainnat}
\bibliography{dissertation}

\end{document}

% vim: lbr
